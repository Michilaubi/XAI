{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8cd1fa-32ee-4625-90fd-f14369a8daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import torch\n",
    "from IMV_LSTM.networks import IMVTensorMultiStepLSTM\n",
    "from IMV_LSTM.model_prep import prepare_multistep_data\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from models.PatchTST_Attention import Model\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "from evaluation.performance import stepwise_errors, get_preds_truths, evaluate\n",
    "import torch\n",
    "from evaluation.randomize_patchtst import (\n",
    "    restore_original_attention,\n",
    "    enable_attention_randomization,\n",
    "    disable_attention_randomization,\n",
    "    patchtst_randomization_check\n",
    ")\n",
    "from evaluation.randomize_imvlstm import imvlstm_attention_randomization_check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ca06a-1d2d-4b48-afa9-3a9b080338c0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5501a-1a5f-47dc-9795-d087e966efef",
   "metadata": {},
   "source": [
    "## PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a45eb-d978-4a1a-9fc0-693eed82b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with pred_len=144, logging to ./logs/LongForecasting/PatchTST_Attention_weather_int_576_144.log ...\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='weather_int_576_144', model='PatchTST_Attention', data='custom', root_path='C:/Users/miche/Documents/PatchTST/PatchTST_supervised/dataset/', data_path='weather_int.csv', features='MS', target='T (degC)', freq='10T', checkpoints='./checkpoints/', seq_len=576, label_len=144, pred_len=144, fc_dropout=0.2, head_dropout=0.0, patch_len=24, stride=12, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=1, embed_type=0, enc_in=7, dec_in=7, c_out=1, d_model=70, n_heads=7, e_layers=3, d_layers=1, d_ff=280, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=True, do_predict=True, num_workers=4, itr=1, train_epochs=100, batch_size=128, patience=25, learning_rate=0.0003, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=True, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use CPU\n",
      ">>>>>>>start training : weather_int_576_144_PatchTST_Attention_custom_ftMS_sl576_ll144_pl144_dm70_nh7_el3_dl1_df280_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 36168\n",
      "val 5127\n",
      "test 10396\n",
      "\titers: 100, epoch: 1 | loss: 0.2297819\n",
      "\tspeed: 5.0550s/iter; left time: 142050.2273s\n",
      "\titers: 200, epoch: 1 | loss: 0.1542028\n",
      "\tspeed: 4.8331s/iter; left time: 135331.4817s\n",
      "Epoch: 1 cost time: 1387.6287443637848\n",
      "Epoch: 1, Steps: 282 | Train Loss: 0.2258008 Vali Loss: 0.1301372 Test Loss: 0.1232091\n",
      "Validation loss decreased (inf --> 0.130137).  Saving model ...\n",
      "Updating learning rate to 0.0003\n"
     ]
    }
   ],
   "source": [
    "%run scripts/PatchTST/weather_int.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d294ff-3738-4d51-9263-e074b17f16dc",
   "metadata": {},
   "source": [
    "## IMV LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036f567-29a9-4ac6-83d0-25fe1a7393d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/IMV_LSTM/weather_int_IMV_LSTM.py \\\n",
    "  --root_path C:/Users/miche/Documents/PatchTST/PatchTST_supervised/dataset/ \\\n",
    "  --data_path weather_int.csv \\\n",
    "  --input_window 576 \\\n",
    "  --forecast_horizon 144 \\\n",
    "  --batch_size 128 \\\n",
    "  --epochs 100 \\\n",
    "  --lr 3e-4 \\\n",
    "  --patience 25 \\\n",
    "  --save_dir ./logs/IMV_weather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5bb14-712e-4c8a-9348-f884cb612186",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c8d28-39dc-4a7a-97c0-51412d36cad1",
   "metadata": {},
   "source": [
    "## Loading Models from Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b68a2-7e84-4e42-8b4e-13cd9a16d92e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prediction Length 10 - PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2343f80d-a078-413a-8719-f654c28d2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # random seed\n",
    "    random_seed = 2021\n",
    "\n",
    "    # basic config\n",
    "    is_training = 0\n",
    "    model_id = 'test'\n",
    "    model = 'PatchTST_Attention'\n",
    "\n",
    "    # data loader\n",
    "    data = 'weather'\n",
    "    root_path = './dataset'\n",
    "    data_path = 'weather_int.csv'\n",
    "    features = 'MS'\n",
    "    target = 'T (degC)'\n",
    "    freq = '10T'\n",
    "    checkpoints = './checkpoints/'\n",
    "\n",
    "    # forecasting task\n",
    "    seq_len = 336\n",
    "    label_len = 10\n",
    "    pred_len = 10\n",
    "\n",
    "    # PatchTST\n",
    "    fc_dropout = 0.2\n",
    "    head_dropout = 0.0\n",
    "    patch_len = 16\n",
    "    stride = 8\n",
    "    padding_patch = 'end'\n",
    "    revin = 1\n",
    "    affine = 0\n",
    "    subtract_last = 0\n",
    "    decomposition = 0\n",
    "    kernel_size = 25\n",
    "    individual = 1  # True\n",
    "\n",
    "    # Formers\n",
    "    embed_type = 0\n",
    "    enc_in = 7\n",
    "    dec_in = 7\n",
    "    c_out = 1\n",
    "    d_model = 70\n",
    "    n_heads = 7\n",
    "    e_layers = 3\n",
    "    d_layers = 1\n",
    "    d_ff = 280\n",
    "    moving_avg = 25\n",
    "    factor = 1\n",
    "    distil = True\n",
    "    dropout = 0.2\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    output_attention = True\n",
    "    do_predict = False\n",
    "\n",
    "    # optimization\n",
    "    num_workers = 4\n",
    "    itr = 1\n",
    "    train_epochs = 100\n",
    "    batch_size = 128\n",
    "    patience = 5\n",
    "    learning_rate = 0.0001\n",
    "    des = 'Exp'\n",
    "    loss = 'mse'\n",
    "    lradj = 'type3'\n",
    "    pct_start = 0.3\n",
    "    use_amp = False\n",
    "\n",
    "    # GPU\n",
    "    use_gpu = True\n",
    "    gpu = 0\n",
    "    use_multi_gpu = False\n",
    "    devices = '0,1,2,3'\n",
    "    test_flop = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056e4953-7a32-4d19-8279-326106e70146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (revin_layer): RevIN()\n",
       "    (padding_patch_layer): ReplicationPad1d((0, 8))\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=16, out_features=70, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (W_K): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (W_V): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=70, out_features=70, bias=True)\n",
       "                (1): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=70, out_features=280, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.2, inplace=False)\n",
       "              (3): Linear(in_features=280, out_features=70, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "          (1): TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (W_K): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (W_V): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=70, out_features=70, bias=True)\n",
       "                (1): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=70, out_features=280, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.2, inplace=False)\n",
       "              (3): Linear(in_features=280, out_features=70, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "          (2): TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (W_K): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (W_V): Linear(in_features=70, out_features=70, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=70, out_features=70, bias=True)\n",
       "                (1): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=70, out_features=280, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.2, inplace=False)\n",
       "              (3): Linear(in_features=280, out_features=70, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (linears): ModuleList(\n",
       "        (0): Linear(in_features=2940, out_features=10, bias=True)\n",
       "        (1): Linear(in_features=2940, out_features=10, bias=True)\n",
       "        (2): Linear(in_features=2940, out_features=10, bias=True)\n",
       "        (3): Linear(in_features=2940, out_features=10, bias=True)\n",
       "        (4): Linear(in_features=2940, out_features=10, bias=True)\n",
       "        (5): Linear(in_features=2940, out_features=10, bias=True)\n",
       "        (6): Linear(in_features=2940, out_features=10, bias=True)\n",
       "      )\n",
       "      (dropouts): ModuleList(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "        (5): Dropout(p=0.0, inplace=False)\n",
       "        (6): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (flattens): ModuleList(\n",
       "        (0): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (1): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (4): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (5): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (6): Flatten(start_dim=-2, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt_path=(\n",
    "    \"scripts/PatchTST/checkpoints/\"\n",
    "    \"weather_int_336_10_PatchTST_Attention_custom_ftMS_sl336_ll10_pl10_dm70_nh7_el3_dl1_df280_fc1_\"\n",
    "    \"ebtimeF_dtTrue_Exp_0/checkpoint.pth\"\n",
    ")\n",
    "ckpt_patch = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "state_dict_patch = ckpt_patch.get(\"model_state_dict\", ckpt_patch)\n",
    "model_patch = Model(args)\n",
    "model_patch = model_patch.float().to(device)\n",
    "model_patch.load_state_dict(state_dict_patch)\n",
    "model_patch.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6974a150-7006-4f76-9317-c9ef2916ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Instantiate your test‐time dataset exactly as during training\n",
    "dataset_test = Dataset_Custom(\n",
    "    root_path   = \"dataset/\",\n",
    "    flag        = \"test\",                 # pulls the test split\n",
    "    size        = [336, 10, 10],          # [seq_len, label_len, pred_len]\n",
    "    features    = \"MS\",\n",
    "    data_path   = \"weather_int.csv\",\n",
    "    target      = \"T (degC)\",\n",
    "    scale       = True,\n",
    "    timeenc     = 0,\n",
    "    freq        = \"10T\",\n",
    ")\n",
    "\n",
    "# 2) Wrap in a DataLoader\n",
    "test_loader_patch = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size  = 128,\n",
    "    shuffle     = False,\n",
    "    num_workers = 4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518d697-b752-48ef-bd4d-867c35ec0d5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prediction Length 10 - IMV-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f6fc63-8ecf-4b8c-ac2f-239820070e9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './imv_best.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2) Load the checkpoint (and automatically map to CPU/GPU):\u001b[39;00m\n\u001b[0;32m     10\u001b[0m ckpt_path_IMV \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./imv_best.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m state_IMV \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path_IMV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m model_IMV\u001b[38;5;241m.\u001b[39mload_state_dict(state_IMV)\n\u001b[0;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/weather_int.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\patchtst\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\patchtst\\lib\\site-packages\\torch\\serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\patchtst\\lib\\site-packages\\torch\\serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './imv_best.pth'"
     ]
    }
   ],
   "source": [
    "# 1) Re-create the model with the same hyper-parameters:\n",
    "model_IMV = IMVTensorMultiStepLSTM(\n",
    "    input_dim     = 7,\n",
    "    output_dim    = 1,\n",
    "    n_units       = 140,\n",
    "    forecast_steps= 10\n",
    ").float().to(device)\n",
    "\n",
    "# 2) Load the checkpoint (and automatically map to CPU/GPU):\n",
    "ckpt_path_IMV = \"./imv_best.pth\"\n",
    "state_IMV = torch.load(ckpt_path_IMV, map_location=device)\n",
    "model_IMV.load_state_dict(state_IMV)\n",
    "\n",
    "df = pd.read_csv(\"dataset/weather_int.csv\")\n",
    "target_weather= 'T (degC)'\n",
    "cols_weather_multi=['p (mbar)', 'Tdew (degC)', 'sh (g/kg)', 'wv (m/s)', 'max. wv (m/s)',\n",
    "       'wd (deg)', 'T (degC)']\n",
    "input_window = 336   # instead of 40\n",
    "forecast_horizon = 10\n",
    "batch_size_weather=128\n",
    "X_train_multi, y_train_multi, \\\n",
    "X_val_multi, y_val_multi, \\\n",
    "X_test_multi, y_test_multi, \\\n",
    "input_scaler_multi, target_scaler_multi = prepare_multistep_data(\n",
    "    df=df,\n",
    "    input_columns=cols_weather_multi,\n",
    "    target_column=target_weather,\n",
    "    input_window=input_window,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    scale_data=True\n",
    ")\n",
    "X_train_t_multi = torch.tensor(X_train_multi, dtype=torch.float32)\n",
    "X_val_t_multi   = torch.tensor(X_val_multi, dtype=torch.float32)\n",
    "X_test_t_multi  = torch.tensor(X_test_multi, dtype=torch.float32)\n",
    "\n",
    "y_train_t_multi = torch.tensor(y_train_multi, dtype=torch.float32)\n",
    "y_val_t_multi   = torch.tensor(y_val_multi, dtype=torch.float32)\n",
    "y_test_t_multi  = torch.tensor(y_test_multi, dtype=torch.float32)\n",
    "\n",
    "\n",
    "test_loader_IMV = DataLoader(\n",
    "    TensorDataset(X_test_t_multi, y_test_t_multi),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size_weather\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f941b-62a9-42f6-b8e7-22364ae35ace",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prediction Length 144 - PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d646a-6ebb-4f11-8d61-09b0076bf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # random seed\n",
    "    random_seed = 2021\n",
    "\n",
    "    # basic config\n",
    "    is_training = 0\n",
    "    model_id = 'weather_int_576_144'\n",
    "    model = 'PatchTST_Attention'\n",
    "\n",
    "    # data loader\n",
    "    data = 'custom'\n",
    "    root_path = './dataset'\n",
    "    data_path = 'weather_int.csv'\n",
    "    features = 'MS'\n",
    "    target = 'T (degC)'\n",
    "    freq = '10T'\n",
    "    checkpoints = './checkpoints/'\n",
    "\n",
    "    # forecasting task\n",
    "    seq_len = 576     # 4 days of history (576 × 10 min)\n",
    "    label_len = 144   # decoder sees last 1 day (144 steps)\n",
    "    pred_len = 144    # forecast horizon = 1 day\n",
    "\n",
    "    # PatchTST-specific\n",
    "    fc_dropout = 0.2\n",
    "    head_dropout = 0.0\n",
    "    patch_len = 24    # 4 h patches (24 × 10 min)\n",
    "    stride = 12       # 50% overlap\n",
    "    padding_patch = 'end'\n",
    "    revin = 1\n",
    "    affine = 0\n",
    "    subtract_last = 0\n",
    "    decomposition = 0\n",
    "    kernel_size = 25\n",
    "    individual = 1  # True\n",
    "\n",
    "    # Transformer backbone\n",
    "    embed_type = 0\n",
    "    enc_in = 7\n",
    "    dec_in = 7\n",
    "    c_out = 1\n",
    "    d_model = 70\n",
    "    n_heads = 7\n",
    "    e_layers = 3\n",
    "    d_layers = 1\n",
    "    d_ff = 280\n",
    "    moving_avg = 25\n",
    "    factor = 1\n",
    "    distil = True\n",
    "    dropout = 0.2\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    output_attention = True\n",
    "    do_predict = False\n",
    "\n",
    "    # optimization\n",
    "    num_workers = 4\n",
    "    itr = 1\n",
    "    train_epochs = 100\n",
    "    batch_size = 128\n",
    "    patience = 25\n",
    "    learning_rate = 1e-4\n",
    "    des = 'Exp'\n",
    "    loss = 'mse'\n",
    "    lradj = 'type3'\n",
    "    pct_start = 0.3\n",
    "    use_amp = False\n",
    "\n",
    "    # GPU\n",
    "    use_gpu = True\n",
    "    gpu = 0\n",
    "    use_multi_gpu = False\n",
    "    devices = '0'\n",
    "    test_flop = False\n",
    "\n",
    "args_long = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8daacfb-d076-4e2d-8555-ce3a1f59669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path_long=(\n",
    "    \"scripts/PatchTST/checkpoints/\"\n",
    "    \"weather_int_336_10_PatchTST_Attention_custom_ftMS_sl336_ll10_pl10_dm70_nh7_el3_dl1_df280_fc1_\"\n",
    "    \"ebtimeF_dtTrue_Exp_0/checkpoint.pth\",\n",
    "    map_location=device\n",
    ")\n",
    "ckpt_patch_long = torch.load(ckpt_path_long, map_location=device)\n",
    "\n",
    "state_dict_patch_long = ckpt_patch_long.get(\"model_state_dict\", ckpt_patch_long)\n",
    "model_patch_long = Model(args_long)\n",
    "model_patch_long = model_patch_long.float().to(device)\n",
    "model_patch_long.load_state_dict(state_dict_patch_long)\n",
    "model_patch_long.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8fca4-4542-4561-bc2d-18847c87bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Instantiate your long-horizon test dataset exactly as during training\n",
    "dataset_test_patch_long = Dataset_Custom(\n",
    "    root_path   = \"dataset/\",           # relative to cwd\n",
    "    flag        = \"test\",               # pulls the test split\n",
    "    size        = [576, 144, 144],      # [seq_len, label_len, pred_len]\n",
    "    features    = \"MS\",\n",
    "    data_path   = \"weather_int.csv\",\n",
    "    target      = \"T (degC)\",\n",
    "    scale       = True,\n",
    "    timeenc     = 0,\n",
    "    freq        = \"10T\",\n",
    ")\n",
    "\n",
    "# 2) Wrap in a DataLoader\n",
    "test_loader_patch_long = DataLoader(\n",
    "    dataset_test_patch_long,\n",
    "    batch_size  = 128,\n",
    "    shuffle     = False,\n",
    "    num_workers = 4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7fe87-c6af-4577-8131-b64309588e57",
   "metadata": {},
   "source": [
    "### Prediction Length 144 - IMV-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48592659-77b8-4cf5-92fb-88bb82dd3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Re-create your long-horizon IMV‐LSTM model\n",
    "model_IMV_long = IMVTensorMultiStepLSTM(\n",
    "    input_dim      = 7,\n",
    "    output_dim     = 1,\n",
    "    n_units        = 140,\n",
    "    forecast_steps = 144        # forecast_horizon_long\n",
    ").float().to(device)\n",
    "\n",
    "# 2) Load the long-horizon checkpoint\n",
    "ckpt_path_IMV_long = \"./imv_best_long.pth\"\n",
    "state_IMV_long     = torch.load(ckpt_path_IMV_long, map_location=device)\n",
    "model_IMV_long.load_state_dict(state_IMV_long)\n",
    "\n",
    "# 3) Prepare your data for the long run\n",
    "df = pd.read_csv(\"dataset/weather_int.csv\")\n",
    "target_weather      = 'T (degC)'\n",
    "cols_weather_multi  = [\n",
    "    'p (mbar)', 'Tdew (degC)', 'sh (g/kg)',\n",
    "    'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'T (degC)'\n",
    "]\n",
    "input_window_long     = 576   # e.g. 4 days of history (576×10 min)\n",
    "forecast_horizon_long = 144   # 1 day ahead (144×10 min)\n",
    "batch_size_weather    = 128\n",
    "\n",
    "X_train_long, y_train_long, \\\n",
    "X_val_long,   y_val_long,   \\\n",
    "X_test_long,  y_test_long,  \\\n",
    "in_scaler_long, out_scaler_long = prepare_multistep_data(\n",
    "    df=df,\n",
    "    input_columns = cols_weather_multi,\n",
    "    target_column = target_weather,\n",
    "    input_window  = input_window_long,\n",
    "    forecast_horizon = forecast_horizon_long,\n",
    "    scale_data    = True\n",
    ")\n",
    "\n",
    "# 4) Tensor‐ify\n",
    "X_test_t_long = torch.tensor(X_test_long, dtype=torch.float32)\n",
    "y_test_t_long = torch.tensor(y_test_long, dtype=torch.float32)\n",
    "\n",
    "# 5) Long‐horizon test DataLoader\n",
    "test_loader_IMV_long = DataLoader(\n",
    "    TensorDataset(X_test_t_long, y_test_t_long),\n",
    "    shuffle     = False,\n",
    "    batch_size  = batch_size_weather,\n",
    "    num_workers = 4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654ad3a-06a3-4c88-8873-66c29b035e68",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512ff37-e1f0-4e44-8dd5-07ff5f7ac9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the target column (same as in your dataset)\n",
    "target = 'T (degC)'\n",
    "data = df[[target]].values\n",
    "\n",
    "# Match train split logic (70% of total rows)\n",
    "num_train = int(len(data) * 0.7)\n",
    "train_data = data[:num_train]\n",
    "\n",
    "# Recreate and fit the scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81fa70-1802-466d-9aaf-614ee0378984",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee73fd4-348e-44f6-a418-e841ba2932a1",
   "metadata": {},
   "source": [
    "### MSE, RMSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ef799-c842-4d37-b5ab-ced43c6fdaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 8) RUN & DISPLAY ─────────────────────────────────────────────────────────\n",
    "results = []\n",
    "for name, mdl, loader in [\n",
    "    (\"PatchTST\",           model_patch,      test_loader_patch),\n",
    "    (\"PatchTST-Long\",      model_patch_long, test_loader_patch_long),\n",
    "    (\"IMV-LSTM\",           model_IMV,        test_loader_IMV),\n",
    "    (\"IMV-LSTM-Long\",      model_IMV_long,   test_loader_IMV_long),\n",
    "]:\n",
    "    metrics = evaluate(mdl, loader, target_channel=-1, scaler=scaler_y)\n",
    "    metrics[\"Model\"] = name\n",
    "    results.append(metrics)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f8fcd-0a94-48a0-8001-de93b4a223bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Collect predictions & truths for both short and long horizons\n",
    "patch_preds,        patch_trues        = get_preds_truths(model_patch,      test_loader_patch,      device)\n",
    "patch_preds_long,   patch_trues_long   = get_preds_truths(model_patch_long, test_loader_patch_long, device)\n",
    "imv_preds,          imv_trues          = get_preds_truths(model_IMV,        test_loader_IMV,        device)\n",
    "imv_preds_long,     imv_trues_long     = get_preds_truths(model_IMV_long,   test_loader_IMV_long,   device)\n",
    "\n",
    "# 2) Compute per-step error\n",
    "patch_rmse,      patch_mae      = stepwise_errors(patch_preds,      patch_trues)\n",
    "patch_long_rmse, patch_long_mae = stepwise_errors(patch_preds_long, patch_trues_long)\n",
    "imv_rmse,        imv_mae        = stepwise_errors(imv_preds,        imv_trues)\n",
    "imv_long_rmse,   imv_long_mae   = stepwise_errors(imv_preds_long,   imv_trues_long)\n",
    "\n",
    "# assume all four series have the same length (pred_len)\n",
    "steps = np.arange(1, len(patch_rmse) + 1)\n",
    "\n",
    "# 3) Plot RMSE comparison\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(steps, patch_rmse,      marker='o', linestyle='-',  label='PatchTST')\n",
    "plt.plot(steps, patch_long_rmse, marker='o', linestyle='--', label='PatchTST-Long')\n",
    "plt.plot(steps, imv_rmse,        marker='s', linestyle='-',  label='IMV-LSTM')\n",
    "plt.plot(steps, imv_long_rmse,   marker='s', linestyle='--', label='IMV-LSTM-Long')\n",
    "plt.xlabel('Forecast Step')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Stepwise RMSE Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Plot MAE comparison\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(steps, patch_mae,      marker='o', linestyle='-',  label='PatchTST')\n",
    "plt.plot(steps, patch_long_mae, marker='o', linestyle='--', label='PatchTST-Long')\n",
    "plt.plot(steps, imv_mae,        marker='s', linestyle='-',  label='IMV-LSTM')\n",
    "plt.plot(steps, imv_long_mae,   marker='s', linestyle='--', label='IMV-LSTM-Long')\n",
    "plt.xlabel('Forecast Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Stepwise MAE Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d62d1-c2a7-46fb-8106-fe8e1bd4fde5",
   "metadata": {},
   "source": [
    "### Attention Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb807cd-57ce-4989-a061-3598f00f7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {\n",
    "    \"short\": dict(\n",
    "        attn_dir       = \"attn/patchtst_short\",\n",
    "        seq_len        = 336,\n",
    "        pred_len       = 10,\n",
    "        num_layers     = 3,\n",
    "        num_batches    = 10,\n",
    "        batch_size     = 128,\n",
    "        num_vars       = 7,\n",
    "        num_heads      = 7,\n",
    "        feature_names  = cols_weather,\n",
    "    ),\n",
    "    \"long\": dict(\n",
    "        attn_dir       = \"attn/patchtst_long\",\n",
    "        seq_len        = 576,\n",
    "        pred_len       = 144,\n",
    "        num_layers     = 3,\n",
    "        num_batches    = 10,\n",
    "        batch_size     = 128,\n",
    "        num_vars       = 7,\n",
    "        num_heads      = 7,\n",
    "        feature_names  = cols_weather,\n",
    "    ),\n",
    "}\n",
    "\n",
    "for run_name, params in runs.items():\n",
    "    plot_patchtst_temporal(\n",
    "        model_name  = \"PatchTST\",\n",
    "        run_name    = run_name,\n",
    "        **params\n",
    "    )\n",
    "    plot_imv_saved_attention(\n",
    "        model_name  = \"IMV-LSTM\",\n",
    "        run_name    = run_name,\n",
    "        **params\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae2498-7807-46b6-8e90-f04b7971afe4",
   "metadata": {},
   "source": [
    "### Randomisation PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57c601-803b-4ba7-84c4-59a637186b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out your two runs\n",
    "restore_original_attention()\n",
    "\n",
    "# 1) quick smoke check on one batch of the short model\n",
    "xb, yb, *_ = next(iter(test_loader_patch))\n",
    "xb = xb.to(device).float()\n",
    "with torch.no_grad():\n",
    "    out = model_patch(xb)\n",
    "print(\"baseline short out shape:\", out.shape)\n",
    "\n",
    "enable_attention_randomization()\n",
    "with torch.no_grad():\n",
    "    out_r = model_patch(xb)\n",
    "print(\"rand-attn short out shape:\", out_r.shape)\n",
    "disable_attention_randomization()\n",
    "\n",
    "# 2) run full randomization tests on both\n",
    "for name, m, loader, pred_len in [\n",
    "    (\"PatchTST-Short\", model_patch,      test_loader_patch,      10),\n",
    "    (\"PatchTST-Long\",  model_patch_long, test_loader_patch_long, 144),\n",
    "]:\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    orig_mse, rand_mse = patchtst_randomization_check(\n",
    "        m, loader, device, pred_len=pred_len\n",
    "    )\n",
    "    print(f\"orig MSE = {orig_mse:.4f}, rand MSE = {rand_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdaaaac-6246-496e-90a0-3b2fe08f1caf",
   "metadata": {},
   "source": [
    "### Randomization IMV-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6bba4-f2ea-4a3e-b44e-4070d5b37d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = [\n",
    "    (\"IMV-LSTM-Short\", model_IMV_short, test_loader_IMV_short, 10),\n",
    "    (\"IMV-LSTM-Long\",  model_IMV_long,  test_loader_IMV_long,  144),\n",
    "]\n",
    "\n",
    "for name, model, loader, pred_len in runs:\n",
    "    model = model.float().to(device).eval()\n",
    "    print(f\"\\n=== {name} (pred_len={pred_len}) ===\")\n",
    "    orig_mse, rand_mse = imvlstm_attention_randomization_check(\n",
    "        model,\n",
    "        loader,\n",
    "        device,\n",
    "        target_channel=-1,\n",
    "        metric=None  # or pass your metric function\n",
    "    )\n",
    "    print(f\"orig MSE = {orig_mse:.4f}   rand MSE = {rand_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67dcd2-0cc2-4644-a31b-c2a9eea1305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import (\n",
    "    imvlstm_attention_randomization_check,\n",
    "    imvlstm_beta_randomization_check\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# assume you have two IMV-LSTM runs defined:\n",
    "#   model_IMV_short, test_loader_IMV_short, pred_len=10\n",
    "#   model_IMV_long,  test_loader_IMV_long,  pred_len=144\n",
    "\n",
    "runs = [\n",
    "    (\"IMV-Short α-rand\", model_IMV_short, test_loader_IMV_short, 10),\n",
    "    (\"IMV-Long  α-rand\", model_IMV_long,  test_loader_IMV_long, 144),\n",
    "    (\"IMV-Short β-rand\", model_IMV_short, test_loader_IMV_short, 10),\n",
    "    (\"IMV-Long  β-rand\", model_IMV_long,  test_loader_IMV_long, 144),\n",
    "]\n",
    "\n",
    "for name, model, loader, pred_len in runs:\n",
    "    model = model.float().to(device).eval()\n",
    "    print(f\"\\n=== {name} (pred_len={pred_len}) ===\")\n",
    "    if \"α\" in name:\n",
    "        orig, rand = imvlstm_attention_randomization_check(\n",
    "            model, loader, device, target_channel=-1\n",
    "        )\n",
    "    else:\n",
    "        orig, rand = imvlstm_beta_randomization_check(\n",
    "            model, loader, device, target_channel=-1\n",
    "        )\n",
    "    print(f\"orig MSE = {orig:.4f}, rand MSE = {rand:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4074c7-2427-4dee-8753-70d4e8649d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707694b7-4322-4ac6-914b-6b21acc6011e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2afdd2-32ad-4821-80a0-99a77dab0c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613f757-e84a-4322-90f8-cae82a8ede52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d76bc3-623d-41b3-9fef-a92f1eaaece0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71daa8-c98b-4b1a-a8a3-ad43bc738e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (patchtst)",
   "language": "python",
   "name": "patchtst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
