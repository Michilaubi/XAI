{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_eval import *\n",
    "from model_prep import * \n",
    "from model_training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = pd.read_csv(\"SML2010/NEW-DATA-1.T15.txt\", sep=' ')\n",
    "#data2 = pd.read_csv(\"SML2010/NEW-DATA-2.T15.txt\", sep=' ')\n",
    "df = pd.read_csv(\"data/weather_int.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_datetime_index(data3, timestamp_col='date', drop_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_weather= 'T (degC)'\n",
    "cols_weather= ['p (mbar)', 'Tdew (degC)', 'sh (g/kg)', 'wv (m/s)', 'max. wv (m/s)',\n",
    "       'wd (deg)', 'T (degC)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original config\n",
    "train_size = 3200\n",
    "val_size = 400\n",
    "depth = 10\n",
    "batch_size = 128\n",
    "prediction_horizon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.zeros((len(data1), depth, len(cols)))\n",
    "y_train1 = np.zeros((len(data1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(cols):\n",
    "    for j in range(depth):\n",
    "        X_train1[:, j, i] = data1[name].shift(depth - j - 1).fillna(method=\"bfill\")\n",
    "y_train1 = data1[target].shift(-prediction_horizon).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train1[depth:-prediction_horizon]\n",
    "y_train1 = y_train1[depth:-prediction_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.zeros((len(data2), depth, len(cols)))\n",
    "y2 = np.zeros((len(data2), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(cols):\n",
    "    for j in range(depth):\n",
    "        X2[:, j, i] = data2[name].shift(depth - j - 1).fillna(method=\"bfill\")\n",
    "y2 = data2[target].shift(-prediction_horizon).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X2[:train_size - len(data1)]\n",
    "y_train2 = y2[:train_size - len(data1)]\n",
    "\n",
    "X_val = X2[train_size - len(data1):train_size - len(data1) + val_size]\n",
    "y_val = y2[train_size - len(data1):train_size - len(data1) + val_size]\n",
    "\n",
    "X_test = X2[train_size - len(data1) + val_size:]\n",
    "y_test = y2[train_size - len(data1) + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train2[depth:]\n",
    "y_train2 = y_train2[depth:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train1, X_train2], axis=0)\n",
    "y_train = np.concatenate([y_train1, y_train2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_min, y_train_min = X_train.min(axis=0), y_train.min(axis=0)\n",
    "X_train_max, y_train_max = X_train.max(axis=0), y_train.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - X_train_min)/(X_train_max - X_train_min + 1e-9)\n",
    "X_val = (X_val - X_train_min)/(X_train_max - X_train_min + 1e-9)\n",
    "X_test = (X_test - X_train_min)/(X_train_max - X_train_min + 1e-9)\n",
    "\n",
    "y_train = (y_train - y_train_min)/(y_train_max - y_train_min + 1e-9)\n",
    "y_val = (y_val - y_train_min)/(y_train_max - y_train_min + 1e-9)\n",
    "y_test = (y_test - y_train_min)/(y_train_max - y_train_min + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.Tensor(X_train)\n",
    "X_val_t = torch.Tensor(X_val)\n",
    "X_test_t = torch.Tensor(X_test)\n",
    "y_train_t = torch.Tensor(y_train)\n",
    "y_val_t = torch.Tensor(y_val.values)\n",
    "y_test_t = torch.Tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters (similar to your old setup)\n",
    "#first run (works well)\n",
    "#depth = 10\n",
    "#batch_size = 128\n",
    "#prediction_horizon = 1\n",
    "#2nd run\n",
    "depth_weather = 10\n",
    "batch_size_weather = 128\n",
    "prediction_horizon_weather = 1\n",
    "\n",
    "# Suppose you have these\n",
    "#cols_weather = [...]  # your input features (list of column names)\n",
    "#target_weather = 'your_target_column'  # your target column name\n",
    "\n",
    "# Size splits\n",
    "train_size_weather = int(0.7 * len(df))   # 70% train\n",
    "val_size_weather = int(0.15 * len(df))    # 15% validation\n",
    "test_size_weather = len(df) - train_size_weather - val_size_weather  # rest for testing\n",
    "\n",
    "# Create input X and y\n",
    "X_weather = np.zeros((len(df), depth_weather, len(cols_weather)))\n",
    "y_weather = np.zeros((len(df), 1))\n",
    "\n",
    "for i, name in enumerate(cols_weather):\n",
    "    for j in range(depth_weather):\n",
    "        X_weather[:, j, i] = df[name].shift(depth_weather - j - 1).fillna(method=\"bfill\")\n",
    "\n",
    "y_weather = df[target_weather].shift(-prediction_horizon_weather).fillna(method='ffill')\n",
    "\n",
    "# Remove the first `depth_weather` samples and last `prediction_horizon_weather` samples\n",
    "X_weather = X_weather[depth_weather:-prediction_horizon_weather]\n",
    "y_weather = y_weather[depth_weather:-prediction_horizon_weather]\n",
    "\n",
    "# Now split into train/val/test\n",
    "X_train_weather = X_weather[:train_size_weather - depth_weather]\n",
    "y_train_weather = y_weather[:train_size_weather - depth_weather]\n",
    "\n",
    "X_val_weather = X_weather[train_size_weather - depth_weather:train_size_weather + val_size_weather - depth_weather]\n",
    "y_val_weather = y_weather[train_size_weather - depth_weather:train_size_weather + val_size_weather - depth_weather]\n",
    "\n",
    "X_test_weather = X_weather[train_size_weather + val_size_weather - depth_weather:]\n",
    "y_test_weather = y_weather[train_size_weather + val_size_weather - depth_weather:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weather.shape, y_train_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weather_min, y_train_weather_min = X_train_weather.min(axis=0), y_train_weather.min(axis=0)\n",
    "X_train_weather_max, y_train_weather_max = X_train_weather.max(axis=0), y_train_weather.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weather = (X_train_weather - X_train_weather_min)/(X_train_weather_max - X_train_weather_min + 1e-9)\n",
    "X_val_weather = (X_val_weather - X_train_weather_min)/(X_train_weather_max - X_train_weather_min + 1e-9)\n",
    "X_test_weather = (X_test_weather - X_train_weather_min)/(X_train_weather_max - X_train_weather_min + 1e-9)\n",
    "\n",
    "y_train_weather = (y_train_weather - y_train_weather_min)/(y_train_weather_max - y_train_weather_min + 1e-9)\n",
    "y_val_weather = (y_val_weather - y_train_weather_min)/(y_train_weather_max - y_train_weather_min + 1e-9)\n",
    "y_test_weather = (y_test_weather - y_train_weather_min)/(y_train_weather_max - y_train_weather_min + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t_weather = torch.Tensor(X_train_weather)\n",
    "X_val_t_weather = torch.Tensor(X_val_weather)\n",
    "X_test_t_weather = torch.Tensor(X_test_weather)\n",
    "y_train_t_weather = torch.Tensor(y_train_weather)\n",
    "y_val_t_weather = torch.Tensor(y_val_weather.values)\n",
    "y_test_t_weather = torch.Tensor(y_test_weather.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_weather = DataLoader(TensorDataset(X_train_t_weather, y_train_t_weather), shuffle=True, batch_size=batch_size_weather)\n",
    "val_loader_weather = DataLoader(TensorDataset(X_val_t_weather, y_val_t_weather), shuffle=False, batch_size=batch_size_weather)\n",
    "test_loader_weather = DataLoader(TensorDataset(X_test_t_weather, y_test_t_weather), shuffle=False, batch_size=batch_size_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_weather = IMVTensorLSTM(X_train_weather.shape[2], 1, 128).to(device)\n",
    "opt_weather = torch.optim.Adam(model_weather.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_scheduler_weather = torch.optim.lr_scheduler.StepLR(opt_weather, 20, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_single_step(model=model_weather,\n",
    "    train_loader=train_loader_weather,\n",
    "    val_loader=val_loader_weather,\n",
    "    optimizer=opt_weather,\n",
    "    scheduler=epoch_scheduler_weather,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    num_epochs=30,\n",
    "    patience=35,\n",
    "    save_path=\"weather_lstm.pt\",\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    y_min=y_train_weather_min,\n",
    "    y_max=y_train_weather_max,\n",
    "    X_train_len=len(X_train_t_weather),\n",
    "    X_val_len=len(X_val_t_weather)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IMVTensorLSTM(X_train.shape[2], 1, 128).cuda()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, 20, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "loss = nn.MSELoss()\n",
    "patience = 35\n",
    "min_val_loss = 9999\n",
    "counter = 0\n",
    "for i in range(epochs):\n",
    "    mse_train = 0\n",
    "    iteration_start = time.monotonic()\n",
    "    for batch_x, batch_y in train_loader :\n",
    "        batch_x = batch_x.cuda()\n",
    "        batch_y = batch_y.cuda()\n",
    "        opt.zero_grad()\n",
    "        y_pred, alphas, betas = model(batch_x)\n",
    "        y_pred = y_pred.squeeze(1)\n",
    "        l = loss(y_pred, batch_y)\n",
    "        l.backward()\n",
    "        mse_train += l.item()*batch_x.shape[0]\n",
    "        opt.step()\n",
    "    epoch_scheduler.step()\n",
    "    with torch.no_grad():\n",
    "        mse_val = 0\n",
    "        preds = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "            output, alphas, betas = model(batch_x)\n",
    "            output = output.squeeze(1)\n",
    "            preds.append(output.detach().cpu().numpy())\n",
    "            true.append(batch_y.detach().cpu().numpy())\n",
    "            mse_val += loss(output, batch_y).item()*batch_x.shape[0]\n",
    "    preds = np.concatenate(preds)\n",
    "    true = np.concatenate(true)\n",
    "    \n",
    "    if min_val_loss > mse_val**0.5:\n",
    "        min_val_loss = mse_val**0.5\n",
    "        print(\"Saving...\")\n",
    "        torch.save(model.state_dict(), \"imv_lstm_sml2010.pt\")\n",
    "        counter = 0\n",
    "    else: \n",
    "        counter += 1\n",
    "    \n",
    "    if counter == patience:\n",
    "        break\n",
    "    print(\"Iter: \", i, \"train: \", (mse_train/len(X_train_t))**0.5, \"val: \", (mse_val/len(X_val_t))**0.5)\n",
    "    iteration_end = time.monotonic()\n",
    "    print(\"Iter time: \", iteration_end - iteration_start)\n",
    "    if(i % 10 == 0):\n",
    "        preds = preds*(y_train_max - y_train_min) + y_train_min\n",
    "        true = true*(y_train_max - y_train_min) + y_train_min\n",
    "        mse = mean_squared_error(true, preds)\n",
    "        mae = mean_absolute_error(true, preds)\n",
    "        print(\"mse: \", mse, \"mae: \", mae)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.plot(preds)\n",
    "        plt.plot(true)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Eval weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weather.load_state_dict(torch.load(\"weather_lstm.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mse_val_weather = 0\n",
    "    preds_weather = []\n",
    "    true_weather = []\n",
    "    alphas_weather = []\n",
    "    betas_weather = []\n",
    "    for batch_x, batch_y in test_loader_weather:\n",
    "        # Move batch to the correct device (GPU or CPU)\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        output_weather, a, b = model_weather(batch_x)\n",
    "        output_weather = output_weather.squeeze(1)\n",
    "    \n",
    "        # Store predictions and ground truth\n",
    "        preds_weather.append(output_weather.detach().cpu().numpy())\n",
    "        true_weather.append(batch_y.detach().cpu().numpy())\n",
    "    \n",
    "        # Store alpha and beta values if needed\n",
    "        alphas_weather.append(a.detach().cpu().numpy())\n",
    "        betas_weather.append(b.detach().cpu().numpy())\n",
    "    \n",
    "        # Compute loss (you can use the same loss function you had before)\n",
    "        mse_val_weather += loss_weather(output_weather, batch_y).item() * batch_x.shape[0]\n",
    "preds_weather = np.concatenate(preds_weather)\n",
    "true_weather = np.concatenate(true_weather)\n",
    "alphas_weather = np.concatenate(alphas_weather)\n",
    "betas_weather = np.concatenate(betas_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_weather = preds_weather*(y_train_weather_max - y_train_weather_min) + y_train_weather_min\n",
    "true_weather = true_weather*(y_train_weather_max - y_train_weather_min) + y_train_weather_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_weather = mean_squared_error(true_weather, preds_weather)\n",
    "mae_weather = mean_absolute_error(true_weather, preds_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_weather, mae_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_weather**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(preds_weather)\n",
    "plt.plot(true_weather)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_weather = alphas_weather.mean(axis=0)\n",
    "betas_weather = betas_weather.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_weather = alphas_weather[..., 0]\n",
    "betas_weather = betas_weather[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_weather = alphas_weather.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "im_weather = ax.imshow(alphas_weather)\n",
    "ax.set_xticks(np.arange(X_train_t_weather.shape[1]))\n",
    "ax.set_yticks(np.arange(len(cols_weather)))\n",
    "ax.set_xticklabels([\"t-\" + str(i) for i in np.arange(X_train_t_weather.shape[1] - 1, -1, -1)])\n",
    "ax.set_yticklabels(cols_weather)\n",
    "for i in range(len(cols_weather)):\n",
    "    for j in range(X_train_t_weather.shape[1]):\n",
    "        text = ax.text(j, i, round(alphas_weather[i, j], 3),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "ax.set_title(\"Importance of features and timesteps\")\n",
    "#fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Feature importance\")\n",
    "plt.bar(range(len(cols_weather)), betas_weather)\n",
    "plt.xticks(range(len(cols_weather)), cols_weather, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mse_val = 0\n",
    "    preds = []\n",
    "    true = []\n",
    "    alphas = []\n",
    "    betas = []\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.cuda()\n",
    "        batch_y = batch_y.cuda()\n",
    "        output, a, b = model(batch_x)\n",
    "        output = output.squeeze(1)\n",
    "        preds.append(output.detach().cpu().numpy())\n",
    "        true.append(batch_y.detach().cpu().numpy())\n",
    "        alphas.append(a.detach().cpu().numpy())\n",
    "        betas.append(b.detach().cpu().numpy())\n",
    "        mse_val += loss(output, batch_y).item()*batch_x.shape[0]\n",
    "preds = np.concatenate(preds)\n",
    "true = np.concatenate(true)\n",
    "alphas = np.concatenate(alphas)\n",
    "betas = np.concatenate(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds*(y_train_max - y_train_min) + y_train_min\n",
    "true = true*(y_train_max - y_train_min) + y_train_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(true, preds)\n",
    "mae = mean_absolute_error(true, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(preds)\n",
    "plt.plot(true)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = alphas.mean(axis=0)\n",
    "betas = betas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = alphas[..., 0]\n",
    "betas = betas[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = alphas.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "im = ax.imshow(alphas)\n",
    "ax.set_xticks(np.arange(X_train_t.shape[1]))\n",
    "ax.set_yticks(np.arange(len(cols)))\n",
    "ax.set_xticklabels([\"t-\"+str(i) for i in np.arange(X_train_t.shape[1], -1, -1)])\n",
    "ax.set_yticklabels(cols)\n",
    "for i in range(len(cols)):\n",
    "    for j in range(X_train_t.shape[1]):\n",
    "        text = ax.text(j, i, round(alphas[i, j], 3),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "ax.set_title(\"Importance of features and timesteps\")\n",
    "#fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Feature importance\")\n",
    "plt.bar(range(len(cols)), betas)\n",
    "plt.xticks(range(len(cols)), cols, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_weather_multi=['p (mbar)', 'Tdew (degC)', 'sh (g/kg)', 'wv (m/s)', 'max. wv (m/s)',\n",
    "       'wd (deg)', 'T (degC)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 576   # instead of 40\n",
    "forecast_horizon = 144\n",
    "\n",
    "X_train_multi, y_train_multi, \\\n",
    "X_val_multi, y_val_multi, \\\n",
    "X_test_multi, y_test_multi, \\\n",
    "input_scaler_multi, target_scaler_multi = prepare_multistep_data(\n",
    "    df=df,\n",
    "    input_columns=cols_weather_multi,\n",
    "    target_column=target_weather,\n",
    "    input_window=input_window,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    scale_data=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t_multi = torch.tensor(X_train_multi, dtype=torch.float32)\n",
    "X_val_t_multi   = torch.tensor(X_val_multi, dtype=torch.float32)\n",
    "X_test_t_multi  = torch.tensor(X_test_multi, dtype=torch.float32)\n",
    "\n",
    "y_train_t_multi = torch.tensor(y_train_multi, dtype=torch.float32)\n",
    "y_val_t_multi   = torch.tensor(y_val_multi, dtype=torch.float32)\n",
    "y_test_t_multi  = torch.tensor(y_test_multi, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=TensorDataset(X_train_t_multi, y_train_t_multi)\n",
    "val_dataset=TensorDataset(X_val_t_multi, y_val_t_multi)\n",
    "test_dataset=TensorDataset(X_test_t_multi, y_test_t_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "param_grid = {\n",
    "    'n_units': [128,256],\n",
    "    'lr': [1e-3, 1e-2],\n",
    "    'forecast_steps': [10],\n",
    "    'batch_size': [128]\n",
    "   # 'attn_dim' :[16, 32]\n",
    "}\n",
    "\n",
    "# Create all combinations\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "param_keys = list(param_grid.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(params, input_dim, output_dim, train_dataset, val_dataset, test_dataset, device):\n",
    "    param_dict = dict(zip(param_keys, params))\n",
    "    \n",
    "    n_units = param_dict['n_units']\n",
    "    lr = param_dict['lr']\n",
    "    forecast_steps = param_dict['forecast_steps']\n",
    "    batch_size = param_dict['batch_size']\n",
    "   # attn_dim = param_dict['attn_dim']\n",
    "\n",
    "    # Correct DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    model = IMVTensorMultiStepLSTM(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        n_units=n_units,\n",
    "        forecast_steps=forecast_steps\n",
    "        #attn_dim=attn_dim\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    model = train_multistep_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=30,\n",
    "    learning_rate=lr,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "    # Now evaluate val and test loss\n",
    "    model.eval()\n",
    "    val_loss, test_loss = 0.0, 0.0\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Validation loss\n",
    "        total_val_loss, total_val_samples = 0, 0\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            preds, _, _ = model(X_val)\n",
    "            loss = loss_fn(preds.squeeze(-1), y_val)\n",
    "            total_val_loss += loss.item() * X_val.size(0)\n",
    "            total_val_samples += X_val.size(0)\n",
    "        val_loss = total_val_loss / total_val_samples\n",
    "    \n",
    "        # Test loss\n",
    "        total_test_loss, total_test_samples = 0, 0\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            preds, _, _ = model(X_test)\n",
    "            loss = loss_fn(preds.squeeze(-1), y_test)\n",
    "            total_test_loss += loss.item() * X_test.size(0)\n",
    "            total_test_samples += X_test.size(0)\n",
    "        test_loss = total_test_loss / total_test_samples\n",
    "    \n",
    "    return val_loss, test_loss, param_dict\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_params = None\n",
    "best_test_loss = None\n",
    "\n",
    "for params in param_combinations:\n",
    "    try:\n",
    "        val_loss, test_loss, param_dict = grid_search_model(\n",
    "            params,\n",
    "            input_dim=len(cols_weather_multi),\n",
    "            output_dim=1,\n",
    "            train_dataset=TensorDataset(X_train_t_multi, y_train_t_multi),\n",
    "            val_dataset=TensorDataset(X_val_t_multi, y_val_t_multi),\n",
    "            test_dataset=TensorDataset(X_test_t_multi, y_test_t_multi),\n",
    "            device=device\n",
    "        )\n",
    "        print(f\"Params: {param_dict} | Val Loss: {val_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_test_loss = test_loss\n",
    "            best_params = param_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with params {params}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Best Parameters Found:\")\n",
    "print(best_params)\n",
    "print(f\"Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Test Loss with Best Params: {best_test_loss:.4f}\")\n",
    "Params: {'n_units': 128, 'lr': 0.01, 'forecast_steps': 10, 'batch_size': 128} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you already have the following model initialized:\n",
    "forecast_horizon=10\n",
    "\n",
    "model_IMV_Con = IMVTensorMultiStepLSTMConicity(input_dim=len(cols_weather_multi), output_dim=1, n_units=128, forecast_steps=forecast_horizon)\n",
    "num_epochs = 20  # You can adjust this as needed\n",
    "learning_rate = 0.01  # Adjust based on experimentation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_con = train_concocity_model(\n",
    "    model=model_IMV_Con,\n",
    "    train_loader=train_loader_multi,\n",
    "    val_loader=val_loader_multi,\n",
    "    num_epochs=20,\n",
    "    learning_rate=0.01,\n",
    "    device=device,\n",
    "    target_scaler=target_scaler_multi  # Optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_conicity_model(model, data_loader, device, target_scaler=None, print_metrics=True):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    alphas_all = []\n",
    "    betas_all = []\n",
    "    conicity_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred, alphas, betas, conicity = model(X_batch)\n",
    "\n",
    "            predictions.append(y_pred.cpu())\n",
    "            targets.append(y_batch.cpu())\n",
    "            alphas_all.append(alphas.cpu())\n",
    "            betas_all.append(betas.cpu())\n",
    "            conicity_scores.append(conicity.item())\n",
    "\n",
    "    predictions = torch.cat(predictions).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "    alphas_all = torch.cat(alphas_all).numpy()\n",
    "    betas_all = torch.cat(betas_all).numpy()\n",
    "\n",
    "    # Optional inverse scaling\n",
    "    if target_scaler is not None:\n",
    "        from copy import deepcopy\n",
    "\n",
    "        def inverse_scale(y):\n",
    "            if y.ndim == 3 and y.shape[-1] == 1:\n",
    "                y = y.squeeze(-1)\n",
    "            y_flat = y.reshape(-1, target_scaler.mean_.shape[0])\n",
    "            return target_scaler.inverse_transform(y_flat).reshape(y.shape)\n",
    "\n",
    "        predictions = inverse_scale(predictions)\n",
    "        targets = inverse_scale(targets)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(targets.flatten(), predictions.flatten())\n",
    "    rmse = mean_squared_error(targets.flatten(), predictions.flatten(), squared=False)\n",
    "    r2 = r2_score(targets.flatten(), predictions.flatten())\n",
    "    avg_conicity = np.mean(conicity_scores)\n",
    "\n",
    "    if print_metrics:\n",
    "        print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R²: {r2:.4f} | Avg Conicity: {avg_conicity:.4f}\")\n",
    "\n",
    "    return predictions, targets, alphas_all, betas_all, {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"Conicity\": avg_conicity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, trues, alphas, betas, metrics = evaluate_conicity_model(\n",
    "    trained_model_con, test_loader_multi, device, target_scaler=target_scaler_multi\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_forecast_vs_truth(preds, trues, index=0):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(trues[index], label='Ground Truth')\n",
    "    plt.plot(preds[index], label='Prediction')\n",
    "    plt.title(f\"Forecast vs. Ground Truth (Sample {index})\")\n",
    "    plt.xlabel(\"Forecast Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_alpha_heatmap(alphas, sample_index=0):\n",
    "    alpha = alphas[sample_index]  # shape: [seq_len, input_dim, n_units]\n",
    "    alpha_mean = alpha.mean(axis=-1)  # shape: [seq_len, input_dim]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(alpha_mean.T, cmap='viridis', cbar=True)\n",
    "    plt.title(f\"Alpha Attention Heatmap (Sample {sample_index})\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Input Feature\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta_per_step(betas, sample_index=0):\n",
    "    beta = betas[sample_index]  # shape: [forecast_steps, input_dim]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(beta.shape[1]):\n",
    "        plt.plot(beta[:, i], label=f\"Feature {i}\")\n",
    "    plt.title(f\"Beta Attention Across Forecast Steps (Sample {sample_index})\")\n",
    "    plt.xlabel(\"Forecast Step\")\n",
    "    plt.ylabel(\"Attention Weight\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conicity_distribution(conicity_scores):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(conicity_scores, bins=20, edgecolor='black')\n",
    "    plt.title(\"Distribution of Conicity Scores\")\n",
    "    plt.xlabel(\"Conicity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conicity_over_epochs(conicity_list):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(conicity_list)\n",
    "    plt.title(\"Mean Conicity Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Conicity\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_forecast_vs_truth(preds, trues, index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_alpha_heatmap(alphas, sample_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta_per_step(betas, sample_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conicity_list = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader_multi:\n",
    "        y_pred, alphas, betas, mean_conicity =model_IMV_Con(X_batch.to(device))\n",
    "        conicity_list.append(mean_conicity.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_conicity_distribution(conicity_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define your loss function builder (from previous steps)\n",
    "def build_loss_function(use_attention_regularization=False, attn_lambda=0.1):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def loss_fn(pred, target, attn_weights=None):\n",
    "        loss = mse_loss(pred, target)\n",
    "        if use_attention_regularization and attn_weights is not None:\n",
    "            var = attn_weights.var(dim=1).mean()\n",
    "            loss = loss + attn_lambda * var\n",
    "        return loss\n",
    "    \n",
    "    return loss_fn\n",
    "forecast_horizon=10\n",
    "# Instantiate your model (make sure it returns attention weights)\n",
    "model_reg = IMVTensorMultiStepLSTM(input_dim=len(cols_weather_multi), output_dim=1, n_units=128, forecast_steps=forecast_horizon)\n",
    "\n",
    "# Build the loss function with attention regularization enabled\n",
    "loss_fn = build_loss_function(use_attention_regularization=True, attn_lambda=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "# Now run training\n",
    "trained_model = train_multistep_model_reg(\n",
    "    model_reg,\n",
    "    train_loader_multi,\n",
    "    val_loader_multi,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn,\n",
    "    target_scaler=target_scaler_multi\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reg_model(model, data_loader, device, target_scaler=None, print_metrics=True):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    alphas_all = []\n",
    "    betas_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred, alphas, betas, _ = model(X_batch)\n",
    "\n",
    "            predictions.append(y_pred.cpu())\n",
    "            targets.append(y_batch.cpu())\n",
    "            alphas_all.append(alphas.cpu())\n",
    "            betas_all.append(betas.cpu())\n",
    "\n",
    "    predictions = torch.cat(predictions).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "    alphas_all = torch.cat(alphas_all).numpy()\n",
    "    betas_all = torch.cat(betas_all).numpy()\n",
    "\n",
    "    # Optional inverse scaling\n",
    "    if target_scaler is not None:\n",
    "        from copy import deepcopy\n",
    "\n",
    "        def inverse_scale(y):\n",
    "            if y.ndim == 3 and y.shape[-1] == 1:\n",
    "                y = y.squeeze(-1)\n",
    "            y_flat = y.reshape(-1, target_scaler.mean_.shape[0])\n",
    "            return target_scaler.inverse_transform(y_flat).reshape(y.shape)\n",
    "\n",
    "        predictions = inverse_scale(predictions)\n",
    "        targets = inverse_scale(targets)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(targets.flatten(), predictions.flatten())\n",
    "    rmse = mean_squared_error(targets.flatten(), predictions.flatten(), squared=False)\n",
    "    r2 = r2_score(targets.flatten(), predictions.flatten())\n",
    "\n",
    "    if print_metrics:\n",
    "        print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "    return predictions, targets, alphas_all, betas_all, {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, trues, alphas, betas, metrics = evaluate_reg_model(\n",
    "    trained_model, test_loader_multi, device, target_scaler=target_scaler_multi\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular IMVLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_weather = 128\n",
    "#prediction_horizon_weather = 1\n",
    "# Training, validation, and test DataLoaders for multistep forecasting\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_t_multi, y_train_t_multi),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size_weather\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(X_val_t_multi, y_val_t_multi),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size_weather\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test_t_multi, y_test_t_multi),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size_weather\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_IMV_model(\n",
    "    model,\n",
    "    data_loader: DataLoader,\n",
    "    device,\n",
    "    target_scaler=None,\n",
    "    save_attention: bool = False,\n",
    "    save_dir: str = \"./attention_test\",\n",
    "    print_metrics: bool = True\n",
    "):\n",
    "    model.eval()\n",
    "    preds_list, trues_list = [], []\n",
    "    alphas_list, betas_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in data_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            y_pred, alphas, betas, _ = model(Xb)    # y_pred: [B, H, 1]\n",
    "            preds_list.append(y_pred.cpu())\n",
    "            trues_list.append(yb.cpu())\n",
    "            alphas_list.append(alphas.cpu())\n",
    "            betas_list.append(betas.cpu())\n",
    "\n",
    "    preds = torch.cat(preds_list).numpy()    # (N, H, 1)\n",
    "    trues = torch.cat(trues_list).numpy()    # (N, H)\n",
    "    alphas = torch.cat(alphas_list).numpy()  # (N, seq_len, D, 1)\n",
    "    betas  = torch.cat(betas_list).numpy()   # (N, H, D, 1)\n",
    "\n",
    "    # 1) Squeeze out that last singleton dimension\n",
    "    preds2 = preds.squeeze(-1)  # → (N, H)\n",
    "    # trues are already (N, H) from your DataLoader\n",
    "\n",
    "    # 2) Inverse‐scale if a scaler was provided\n",
    "    if target_scaler is not None:\n",
    "        # This assumes target_scaler was fit on shape (N_train, H)\n",
    "        preds2 = target_scaler.inverse_transform(preds2)\n",
    "        trues = target_scaler.inverse_transform(trues)\n",
    "\n",
    "    # 3) Save the raw attention (before flattening) if requested\n",
    "    if save_attention:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        np.save(os.path.join(save_dir, \"alphas.npy\"), alphas)\n",
    "        np.save(os.path.join(save_dir, \"betas.npy\"),  betas)\n",
    "        if print_metrics:\n",
    "            print(f\"✅ Saved attention maps to {save_dir}/{{alphas.npy,betas.npy}}\")\n",
    "\n",
    "    # 4) Compute metrics\n",
    "    y_pred_flat = preds2.reshape(-1)\n",
    "    y_true_flat = trues.reshape(-1)\n",
    "\n",
    "    mae  = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "    rmse = mean_squared_error(y_true_flat, y_pred_flat)\n",
    "    r2   = r2_score(y_true_flat, y_pred_flat)\n",
    "\n",
    "    if print_metrics:\n",
    "        print(f\"→ Eval: MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "    return preds2, trues, alphas, betas, {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "\n",
    "def train_and_evaluate_IMV(\n",
    "    model,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    device,\n",
    "    target_scaler,\n",
    "    num_epochs: int = 100,\n",
    "    lr: float = 3e-4,\n",
    "    patience: int = 25,\n",
    "    save_attention_dir: str = \"./imv_attention\"\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds, all_trues = [], []\n",
    "\n",
    "        print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
    "\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, _, _, _ = model(Xb)       # [B, H, 1]\n",
    "            y_pred = y_pred.squeeze(-1)       # [B, H]\n",
    "\n",
    "            loss = criterion(y_pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item() * Xb.size(0))\n",
    "            all_preds.append(y_pred.detach().cpu().numpy())\n",
    "            all_trues.append(yb.detach().cpu().numpy())\n",
    "\n",
    "        # Aggregate training metrics\n",
    "        avg_train_loss = sum(train_losses) / len(train_loader.dataset)\n",
    "        preds_flat = np.concatenate(all_preds, axis=0).reshape(-1)\n",
    "        trues_flat = np.concatenate(all_trues, axis=0).reshape(-1)\n",
    "\n",
    "        train_mae  = mean_absolute_error(trues_flat, preds_flat)\n",
    "        train_rmse = mean_squared_error(trues_flat, preds_flat)\n",
    "        train_r2   = r2_score(trues_flat, preds_flat)\n",
    "\n",
    "        print(f\"Train → Loss: {avg_train_loss:.4f}, MAE: {train_mae:.4f}, \"\n",
    "              f\"RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "\n",
    "        # ——— Validation ———\n",
    "        _, _, _, _, val_metrics = evaluate_IMV_model(\n",
    "            model, val_loader, device,\n",
    "            target_scaler=target_scaler,\n",
    "            save_attention=False,\n",
    "            print_metrics=True\n",
    "        )\n",
    "        val_rmse = val_metrics[\"RMSE\"]\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_rmse < best_val_rmse * 0.995:  # require 0.5% improvement\n",
    "            best_val_rmse = val_rmse\n",
    "            patience_ctr = 0\n",
    "            torch.save(model.state_dict(), \"imv_best.pth\")\n",
    "            print(\"🎉 New best model saved.\")\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            print(f\"No improvement → patience {patience_ctr}/{patience}\")\n",
    "            if patience_ctr >= patience:\n",
    "                print(\"⏱ Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Load best and do final eval on test set (saving its attention maps)\n",
    "    model.load_state_dict(torch.load(\"imv_best.pth\"))\n",
    "    return evaluate_IMV_model(\n",
    "        model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        target_scaler=target_scaler,\n",
    "        save_attention=True,\n",
    "        save_dir=save_attention_dir,\n",
    "        print_metrics=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Instantiate & move to device\n",
    "model_IMV = IMVTensorMultiStepLSTM(\n",
    "    input_dim    = len(cols_weather_multi),  # number of input vars\n",
    "    output_dim   = 1,                        # forecasting one target\n",
    "    n_units      = 140,                      # hidden dimension\n",
    "    forecast_steps = 144                      # 10-step horizon\n",
    ").to(device)\n",
    "\n",
    "# 1) Train + save best IMV‐LSTM on val‐split (and save attention for val)\n",
    "preds_val, trues_val, alphas_val, betas_val, metrics_val = train_and_evaluate_IMV(\n",
    "    model_IMV,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    device,\n",
    "    target_scaler_multi,\n",
    "    num_epochs=100,\n",
    "    lr=3e-4,\n",
    "    patience=25,\n",
    "    save_attention_dir=\"./imv_attention_val\"   # saves alphas.npy & betas.npy for val\n",
    ")\n",
    "\n",
    "# 2) Now evaluate on your test set\n",
    "preds_test, trues_test, alphas_test, betas_test, metrics_test = evaluate_IMV_model(\n",
    "    model_IMV,\n",
    "    test_loader,\n",
    "    device,\n",
    "    target_scaler=target_scaler_multi,\n",
    "    save_attention=True,          # save the test‐set attention maps too\n",
    "    save_dir=\"./imv_attention_test\",\n",
    "    print_metrics=True\n",
    ")\n",
    "\n",
    "# 3) metrics_test now holds your test MAE/RMSE/R²\n",
    "print(\"Test Metrics:\", metrics_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/1 ===\n",
      "Train → Loss: 0.4910, MAE: 0.5554, RMSE: 0.4910, R²: 0.5090\n",
      "→ Eval: MAE=4.5679, RMSE=30.7695, R²=-0.1433\n",
      "🎉 New best model saved.\n",
      "✅ Saved attention maps to ./logs/IMV_weather\\attention_val/{alphas.npy,betas.npy}\n",
      "→ Eval: MAE=3.7295, RMSE=18.9554, R²=-0.3037\n",
      "✅ Saved attention maps to ./logs/IMV_weather\\attention_test/{alphas.npy,betas.npy}\n",
      "→ Eval: MAE=3.7295, RMSE=18.9554, R²=-0.3037\n",
      "Test metrics: {'MAE': 3.729501724243164, 'RMSE': 18.955427169799805, 'R2': -0.30368101596832275}\n"
     ]
    }
   ],
   "source": [
    "%run scripts/PatchTST/weather_int_IMV_LSTM.py \\\n",
    "  --root_path C:/Users/miche/Documents/PatchTST/PatchTST_supervised/dataset/ \\\n",
    "  --data_path weather_int.csv \\\n",
    "  --input_window 576 \\\n",
    "  --forecast_horizon 144 \\\n",
    "  --batch_size 128 \\\n",
    "  --epochs 100 \\\n",
    "  --lr 3e-4 \\\n",
    "  --patience 25 \\\n",
    "  --save_dir ./logs/IMV_weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Direct multi-step forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_direct_imv_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_losses, train_mae, train_rmse = [], [], []\n",
    "\n",
    "        for xb, yb in tqdm(train_loader, desc=f\"[Epoch {epoch:03d}] Training\"):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, _, _, _ = model(xb)\n",
    "            loss = criterion(y_pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_mae.append(torch.mean(torch.abs(y_pred - yb)).item())\n",
    "            train_rmse.append(torch.sqrt(F.mse_loss(y_pred, yb)).item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses, val_mae, val_rmse = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                y_pred, _, _, _ = model(xb)\n",
    "                loss = criterion(y_pred, yb)\n",
    "\n",
    "                val_losses.append(loss.item())\n",
    "                val_mae.append(torch.mean(torch.abs(y_pred - yb)).item())\n",
    "                val_rmse.append(torch.sqrt(F.mse_loss(y_pred, yb)).item())\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Train Loss: {np.mean(train_losses):.4f} | Val Loss: {np.mean(val_losses):.4f} | \"\n",
    "            f\"Train MAE: {np.mean(train_mae):.4f} | Val MAE: {np.mean(val_mae):.4f} | \"\n",
    "            f\"Train RMSE: {np.mean(train_rmse):.4f} | Val RMSE: {np.mean(val_rmse):.4f}\"\n",
    "        )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_multi_dir, y_train_multi_dir, \\\n",
    "X_val_multi_dir, y_val_multi_dir, \\\n",
    "X_test_multi_dir, y_test_multi_dir, \\\n",
    "input_scaler_multi_dir, target_scaler_multi_dir = prepare_multistep_data(\n",
    "    df=df,\n",
    "    input_columns=cols_weather_multi,\n",
    "    target_column=target_weather,\n",
    "    input_window=10,\n",
    "    forecast_horizon=3,\n",
    "    scale_data=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t_multi_dir = torch.tensor(X_train_multi_dir, dtype=torch.float32)\n",
    "X_val_t_multi_dir   = torch.tensor(X_val_multi_dir, dtype=torch.float32)\n",
    "X_test_t_multi_dir  = torch.tensor(X_test_multi_dir, dtype=torch.float32)\n",
    "\n",
    "y_train_t_multi_dir = torch.tensor(y_train_multi_dir, dtype=torch.float32)\n",
    "y_val_t_multi_dir   = torch.tensor(y_val_multi_dir, dtype=torch.float32)\n",
    "y_test_t_multi_dir  = torch.tensor(y_test_multi_dir, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd run\n",
    "#depth_weather = 10\n",
    "batch_size_weather = 128\n",
    "#prediction_horizon_weather = 1\n",
    "# Training, validation, and test DataLoaders for multistep forecasting\n",
    "train_loader_multi_dir = DataLoader(\n",
    "    TensorDataset(X_train_t_multi_dir, y_train_t_multi_dir),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size_weather\n",
    ")\n",
    "\n",
    "val_loader_multi_dir = DataLoader(\n",
    "    TensorDataset(X_val_t_multi_dir, y_val_t_multi_dir),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size_weather\n",
    ")\n",
    "\n",
    "test_loader_multi_dir = DataLoader(\n",
    "    TensorDataset(X_test_t_multi_dir, y_test_t_multi_dir),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size_weather\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_model = DirectIMVTensorLSTM(\n",
    "    input_dim=X_train_t_multi_dir.shape[2],  # number of input features\n",
    "    output_dim=3,                        # forecast_horizon (i.e., 3 steps ahead)\n",
    "    n_units=64                           # hidden units (tunable)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_direct_imv_model(\n",
    "    model=direct_model,\n",
    "    train_loader=train_loader_multi_dir,\n",
    "    val_loader=val_loader_multi_dir,\n",
    "    num_epochs=20,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import model_prep\n",
    "importlib.reload(model_prep)\n",
    "# Assuming predictions and targets are scaled arrays, and target_scaler_multi is the fitted scaler\n",
    "print(\"Before inverse scaling:\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "\n",
    "preds_unscaled = inverse_scale_predictions(predictions, target_scaler_multi)\n",
    "targets_unscaled = inverse_scale_predictions(targets, target_scaler_multi)\n",
    "\n",
    "# Check the unscaled results' shape\n",
    "print(\"After inverse scaling:\")\n",
    "print(f\"Predictions unscaled shape: {preds_unscaled.shape}\")\n",
    "print(f\"Targets unscaled shape: {targets_unscaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now you can use the plotting functions to visualize:\n",
    "plot_predictions(targets_unscaled, preds_unscaled, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(targets_unscaled, preds_unscaled, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_weights(alphas, betas, sample_idx=0, input_var_names=cols_weather_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta_attention_over_forecast_steps(betas, sample_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_vs_actual_per_step(predictions, targets, sample_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_forecast_vs_actual_per_step(preds_unscaled, targets_unscaled, sample_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_forecast_vs_actual_per_step(preds_unscaled, targets_unscaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_steps(preds_unscaled, targets_unscaled, forecast_horizon=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_steps_with_error_bands(preds_unscaled, targets_unscaled, forecast_horizon=10, subset_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Randomly select 10 samples\n",
    "plot_forecast_steps_with_error_bands(preds_unscaled, targets_unscaled, forecast_horizon=forecast_horizon, subset_size=10, random_subset=True)\n",
    "\n",
    "# Example 2: Manually select specific samples\n",
    "#plot_forecast_steps_with_error_bands(predictions, targets, forecast_horizon=5, subset_size=10, selected_indices=[0, 5, 10])\n",
    "\n",
    "# Example 3: Select samples based on a condition (e.g., t+1 > 0.5)\n",
    "#plot_forecast_steps_with_error_bands(predictions, targets, forecast_horizon=5, subset_size=10, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Select samples based on a condition (e.g., t+1 > 0.5)\n",
    "plot_forecast_steps_with_error_bands_deviation(preds_unscaled, targets_unscaled, forecast_horizon=forecast_horizon, subset_size=10, deviation_threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Plot only instances where the mean deviation across forecast steps exceeds 0.5\n",
    "#plot_forecast_steps_with_error_bands(predictions, targets, forecast_horizon=5, subset_size=10, deviation_threshold=0.5)\n",
    "\n",
    "# Example 2: Plot only instances where the mean deviation across forecast steps exceeds 1.0\n",
    "plot_forecast_steps_with_error_bands(preds_unscaled, targets_unscaled, forecast_horizon=10, subset_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dele, y_train_dele, \\\n",
    "X_val_dele, y_val_dele, \\\n",
    "X_test_dele, y_test_dele, \\\n",
    "input_scaler_dele, target_scaler_dele = prepare_multistep_data(\n",
    "    df=df,\n",
    "    input_columns=cols_weather_multi,\n",
    "    target_column=target_weather,\n",
    "    input_window=14,\n",
    "    forecast_horizon=10,\n",
    "    scale_data=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t_dele = torch.tensor(X_train_dele, dtype=torch.float32)\n",
    "X_val_t_dele   = torch.tensor(X_val_dele, dtype=torch.float32)\n",
    "X_test_t_dele  = torch.tensor(X_test_dele, dtype=torch.float32)\n",
    "\n",
    "y_train_t_dele = torch.tensor(y_train_dele, dtype=torch.float32)\n",
    "y_val_t_dele   = torch.tensor(y_val_dele, dtype=torch.float32)\n",
    "y_test_t_dele  = torch.tensor(y_test_dele, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd run\n",
    "#depth_weather = 10\n",
    "batch_size_dele = 128\n",
    "#prediction_horizon_weather = 1\n",
    "# Training, validation, and test DataLoaders for multistep forecasting\n",
    "train_loader_dele = DataLoader(\n",
    "    TensorDataset(X_train_t_dele, y_train_t_dele),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size_dele\n",
    ")\n",
    "\n",
    "val_loader_dele = DataLoader(\n",
    "    TensorDataset(X_val_t_dele, y_val_t_dele),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size_dele\n",
    ")\n",
    "\n",
    "test_loader_dele = DataLoader(\n",
    "    TensorDataset(X_test_t_dele, y_test_t_dele),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size_dele\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_dele(model, test_loader, forecast_steps, target_scaler, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            pred, _, _ = model(x, device)\n",
    "            pred = pred[:, -forecast_steps:, 0]  # shape: [batch_size, forecast_steps]\n",
    "\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_targets.append(y.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    # Inverse transform the scaled data\n",
    "    all_preds_unscaled = inverse_scale_predictions(all_preds, target_scaler)\n",
    "    all_targets_unscaled = inverse_scale_predictions(all_targets, target_scaler)\n",
    "\n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(all_targets_unscaled, all_preds_unscaled)\n",
    "    mae = mean_absolute_error(all_targets_unscaled, all_preds_unscaled)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f\"Evaluation Metrics on Unscaled Test Data:\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return all_preds_unscaled, all_targets_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example params (you need to adjust based on your input data)\n",
    "Delelstm_params = {\n",
    "    'input_dim': X_train_t_dele.shape[2],     # e.g. number of variables in input data\n",
    "    'n_units': 64,                 # hidden size for tensor LSTM\n",
    "    'time_depth':X_train_t_dele.shape[1],  # how many time steps input has\n",
    "    'output_dim': 1,               # typically 1 for forecasting a single variable\n",
    "    'N_units': 64                  # hidden size for vanilla LSTM\n",
    "}\n",
    "\n",
    "short = 2  # or another appropriate threshold used in your logic\n",
    "\n",
    "# Instantiate the model\n",
    "model_delelstm = Delelstm(Delelstm_params, short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_delelstm.to(device)\n",
    "\n",
    "optimizer_delelstm = torch.optim.Adam(model_delelstm.parameters(), lr=0.001)\n",
    "criterion_delelstm = torch.nn.MSELoss()  # or MAE, depending on your metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train_t_dele.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 30\n",
    "forecast_steps = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_delelstm.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    loop = tqdm(train_loader_dele, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    \n",
    "    for x, y in loop:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer_delelstm.zero_grad()\n",
    "\n",
    "        pred, unorm, _ = model_delelstm(x, device)\n",
    "        pred = pred[:, -forecast_steps:, 0]  # [batch_size, forecast_steps]\n",
    "\n",
    "        loss = criterion_delelstm(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer_delelstm.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Collect predictions and targets for metrics\n",
    "        all_preds.append(pred.detach().cpu().numpy())\n",
    "        all_targets.append(y.detach().cpu().numpy())\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Compute metrics\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    rmse = mean_squared_error(all_targets, all_preds, squared=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "forecast_steps = 10\n",
    "model_delelstm.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in test_loader_dele:\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "        pred, _, _ = model_delelstm(x_test, device)\n",
    "        pred = pred[:, -forecast_steps:, 0]  # same slicing as training\n",
    "\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_targets.append(y_test.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(all_targets, all_preds)\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"\\n📊 Test Metrics:\")\n",
    "print(f\"  - MAE:  {mae:.4f}\")\n",
    "print(f\"  - RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first sample in the test set\n",
    "plt.plot(all_targets[0], label='True')\n",
    "plt.plot(all_preds[0], label='Predicted')\n",
    "plt.title(\"Forecast vs Actual (first test sample)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_unscaled, all_targets_unscaled = evaluate_model_dele(\n",
    "    model_delelstm,\n",
    "    test_loader_dele,\n",
    "    forecast_steps=10,\n",
    "    target_scaler=target_scaler_dele,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(all_targets_unscaled[0], label=\"True\")\n",
    "plt.plot(all_preds_unscaled[0], label=\"Predicted\")\n",
    "plt.title(\"Multistep Forecast (first test sample)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import copy\n",
    "\n",
    "def randomization_sanity_check(model, data_loader, device=device):\n",
    "    model_random = copy.deepcopy(model).to(device)\n",
    "\n",
    "    # Randomize model weights\n",
    "    for param in model_random.parameters():\n",
    "        if param.requires_grad:\n",
    "            torch.nn.init.normal_(param, mean=0, std=0.1)\n",
    "\n",
    "    model.eval()\n",
    "    model_random.eval()\n",
    "\n",
    "    similarities_alpha = []\n",
    "    similarities_beta = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "\n",
    "            _, alpha_orig, beta_orig = model(x_batch)\n",
    "            _, alpha_rand, beta_rand = model_random(x_batch)\n",
    "\n",
    "            # Flatten\n",
    "            alpha_orig = alpha_orig.reshape(-1).to(device).numpy()\n",
    "            alpha_rand = alpha_rand.reshape(-1).to(device).numpy()\n",
    "            beta_orig = beta_orig.reshape(-1).to(device).numpy()\n",
    "            beta_rand = beta_rand.reshape(-1).to(device).numpy()\n",
    "\n",
    "            # Compute Spearman similarity\n",
    "            s_alpha, _ = spearmanr(alpha_orig, alpha_rand)\n",
    "            s_beta, _ = spearmanr(beta_orig, beta_rand)\n",
    "\n",
    "            similarities_alpha.append(s_alpha)\n",
    "            similarities_beta.append(s_beta)\n",
    "\n",
    "    print(f\"Avg Spearman alpha similarity: {sum(similarities_alpha)/len(similarities_alpha):.4f}\")\n",
    "    print(f\"Avg Spearman beta similarity:  {sum(similarities_beta)/len(similarities_beta):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion_check(model, data_loader, k_ratio=0.1, use='alpha', device=device):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    errors_full, errors_deleted = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            forecasts, alphas, betas = model(x_batch)\n",
    "            loss_full = criterion(forecasts.squeeze(-1), y_batch)\n",
    "            errors_full.append(loss_full.item())\n",
    "\n",
    "            # Determine importance\n",
    "            if use == 'alpha':\n",
    "                importance = alphas.mean(dim=1)  # average across time: [batch, vars, units=1]\n",
    "            elif use == 'beta':\n",
    "                importance = betas.mean(dim=(1, 2))  # average over steps and units\n",
    "            else:\n",
    "                raise ValueError(\"use must be 'alpha' or 'beta'\")\n",
    "\n",
    "            importance_scores = importance.detach().to(device).numpy()\n",
    "            x_batch_mod = x_batch.clone()\n",
    "\n",
    "            for i in range(x_batch.size(0)):\n",
    "                imp = importance_scores[i]\n",
    "                topk = int(len(imp) * k_ratio)\n",
    "                topk_idx = imp.argsort()[-topk:]\n",
    "                x_batch_mod[i, :, topk_idx] = 0.0  # zero out top-k variables\n",
    "\n",
    "            forecasts_mod, _, _ = model(x_batch_mod)\n",
    "            loss_deleted = criterion(forecasts_mod.squeeze(-1), y_batch)\n",
    "            errors_deleted.append(loss_deleted.item())\n",
    "\n",
    "    print(f\"Original Avg Loss: {sum(errors_full)/len(errors_full):.4f}\")\n",
    "    print(f\"After Deletion Avg Loss: {sum(errors_deleted)/len(errors_deleted):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def random_deletion_sanity_check(model, data_loader, top_k=3, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Randomly select top_k variable indices per batch\n",
    "            batch_size, seq_len, input_dim = x_batch.shape\n",
    "            mask = torch.ones_like(x_batch)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                random_vars = torch.randperm(input_dim)[:top_k]\n",
    "                mask[i, :, random_vars] = 0  # zero out random variables\n",
    "\n",
    "            x_deleted = x_batch * mask  # apply mask\n",
    "            forecasts, _, _ = model(x_deleted)\n",
    "            loss = criterion(forecasts.squeeze(-1), y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"🔀 Random Deletion Avg Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_IMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "#randomization_sanity_check(model_inter, test_loader_multi)\n",
    "\n",
    "randomization_sanity_check(model_IMV, test_loader_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha attention (temporal variable importance) has very low similarity to a random model → this suggests your model has learned meaningful temporal patterns.\n",
    "\n",
    "Beta attention (variable-level forecasting importance) has some structure, but moderate similarity suggests it may still be influenced by spurious patterns or has weaker interpretability than alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletion_check(model_inter, test_loader_multi, k_ratio=0.2, use='alpha')  # or use='beta'\n",
    "deletion_check(model_IMV, test_loader_multi, k_ratio=0.2, use='alpha')  # or use='beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletion_check(model_IMV, test_loader_multi, k_ratio=0.2, use='beta')  # or use='beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_deletion_sanity_check(model_inter, test_loader_multi, top_k=3, device=device)\n",
    "random_deletion_sanity_check(model_IMV, test_loader_multi, top_k=3, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Scenario            | Avg Loss | Interpretation                                                       |\n",
    "| ------------------- | -------- | -------------------------------------------------------------------- |\n",
    "| **Original**        | `0.0107` | Normal test loss using the model with learned attention weights.     |\n",
    "| **After Deletion**  | `0.0129` | Loss after **removing** (zeroing or masking) top-attended variables. |\n",
    "| **Random Deletion** | `0.0925` | Loss after **randomly** removing variables (not based on attention). |\n",
    "1. Deletion Check Validity\n",
    "Deleting top-attended variables increased the loss slightly (0.0107 → 0.0129).\n",
    "\n",
    "Random deletions increased the loss significantly (0.0107 → 0.0925).\n",
    "\n",
    "This suggests:\n",
    "\n",
    "The model indeed relies more heavily on variables it gives high attention to.\n",
    "\n",
    "But deleting them doesn't cause catastrophic failure → possibly due to redundancy in your input features (other variables can partially compensate).\n",
    "\n",
    "2. Trustworthiness of Attention\n",
    "The fact that attention-guided deletion hurts performance less than random deletion shows that attention is pointing to genuinely important variables.\n",
    "\n",
    "If deleting top-attended variables had no effect, the attention would be untrustworthy.\n",
    "\n",
    "If deleting them caused massive degradation, it might mean your model is overly reliant on just a few variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_alpha_heatmap(alphas, var_names=None, title=\"Alpha Attention Heatmap\"):\n",
    "    # alphas shape: [batch, time, input_dim, 1] → squeeze last dim\n",
    "    alpha_vals = alphas.squeeze(-1).mean(dim=0).detach().to(device).numpy()  # [time, input_dim]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(alpha_vals.T, cmap=\"YlGnBu\", xticklabels=True, yticklabels=var_names if var_names else True)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Variable\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta_heatmap(betas, var_names=None, title=\"Beta Attention Heatmap\"):\n",
    "    # betas shape: [batch, forecast_steps, input_dim, 1] → squeeze and average\n",
    "    beta_vals = betas.squeeze(-1).mean(dim=0).detach().to(device).numpy()  # [forecast_steps, input_dim]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(beta_vals.T, cmap=\"OrRd\", xticklabels=True, yticklabels=var_names if var_names else True)\n",
    "    plt.xlabel(\"Forecast Step\")\n",
    "    plt.ylabel(\"Variable\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass on a test batch\n",
    "x_batch, _ = next(iter(test_loader_multi))\n",
    "x_batch = x_batch.to(device)\n",
    "_, alphas, betas = model_IMV(x_batch)\n",
    "\n",
    "plot_alpha_heatmap(alphas, var_names=cols_weather_multi)\n",
    "plot_beta_heatmap(betas, var_names=cols_weather_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_weather_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "def evaluate_stability_fidelity(model, data_loader, device='cpu', epsilon=1e-2):\n",
    "    model.eval()\n",
    "    stability_scores = []\n",
    "    fidelity_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "\n",
    "            # Original predictions and explanations\n",
    "            preds_orig, alphas_orig, _ = model(x_batch)\n",
    "\n",
    "            # Perturb input slightly\n",
    "            noise = torch.randn_like(x_batch) * epsilon\n",
    "            x_perturbed = x_batch + noise\n",
    "\n",
    "            # Perturbed predictions and explanations\n",
    "            preds_perturbed, alphas_perturbed, _ = model(x_perturbed)\n",
    "\n",
    "            # ========== Stability: Spearman similarity between alphas ==========\n",
    "            for alpha_o, alpha_p in zip(alphas_orig, alphas_perturbed):\n",
    "                s, _ = spearmanr(alpha_o.cpu().numpy().flatten(), alpha_p.cpu().numpy().flatten())\n",
    "                if not np.isnan(s):  # Filter out NaNs from constant vectors\n",
    "                    stability_scores.append(s)\n",
    "\n",
    "            # ========== Fidelity: MSE between predictions ==========\n",
    "            fidelity = mse_loss(preds_orig, preds_perturbed).item()\n",
    "            fidelity_scores.append(fidelity)\n",
    "\n",
    "    print(f\"Avg Stability (Spearman): {np.mean(stability_scores):.4f}\")\n",
    "    print(f\"Avg Fidelity (MSE):       {np.mean(fidelity_scores):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run on your test/val loader\n",
    "evaluate_stability_fidelity(model_IMV, test_loader_multi, device=device,epsilon=1e-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_similarity_at_epsilon(model, data_loader, epsilon, device='cpu'):\n",
    "    model.eval()\n",
    "    spearman_scores = []\n",
    "    cosine_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "\n",
    "            # Original explanations (alphas)\n",
    "            _, alphas_orig, _ = model(x_batch)\n",
    "\n",
    "            # Perturb inputs\n",
    "            noise = torch.randn_like(x_batch) * epsilon\n",
    "            x_perturbed = x_batch + noise\n",
    "\n",
    "            # Perturbed explanations\n",
    "            _, alphas_perturbed, _ = model(x_perturbed)\n",
    "\n",
    "            # Compare each example in batch\n",
    "            for alpha_o, alpha_p in zip(alphas_orig, alphas_perturbed):\n",
    "                alpha_o_np = alpha_o.cpu().numpy().flatten()\n",
    "                alpha_p_np = alpha_p.cpu().numpy().flatten()\n",
    "\n",
    "                # Spearman\n",
    "                s, _ = spearmanr(alpha_o_np, alpha_p_np)\n",
    "                if not np.isnan(s):\n",
    "                    spearman_scores.append(s)\n",
    "\n",
    "                # Cosine similarity requires 2D arrays: reshape (1, -1)\n",
    "                cos_sim = cosine_similarity(alpha_o_np.reshape(1, -1), alpha_p_np.reshape(1, -1))[0][0]\n",
    "                cosine_scores.append(cos_sim)\n",
    "\n",
    "    return np.mean(spearman_scores), np.mean(cosine_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epsilons = np.linspace(0, 0.2, 21)  # from 0 to 0.2 in 0.01 steps\n",
    "spearman_means = []\n",
    "cosine_means = []\n",
    "\n",
    "for e in epsilons:\n",
    "    sp, cs = evaluate_similarity_at_epsilon(model_IMV, test_loader_multi, epsilon=e, device=device)\n",
    "    spearman_means.append(sp)\n",
    "    cosine_means.append(cs)\n",
    "    print(f\"Epsilon={e:.3f}: Spearman={sp:.4f}, Cosine={cs:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interaction_heatmap(interactions, var_names=None, title=\"Variable Interaction Heatmap\"):\n",
    "    # interactions shape: [batch, time, input_dim, input_dim]\n",
    "    # Average over batch and time for a summary heatmap\n",
    "    interaction_avg = interactions.mean(dim=0).mean(dim=0).detach().to(device).numpy()  # [input_dim, input_dim]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        interaction_avg,\n",
    "        cmap=\"coolwarm\",\n",
    "        xticklabels=var_names if var_names else True,\n",
    "        yticklabels=var_names if var_names else True,\n",
    "        square=True,\n",
    "        annot=True,  # Optional: show numbers in cells\n",
    "        fmt=\".2f\"\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Variable (Key)\")\n",
    "    plt.ylabel(\"Variable (Query)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts, alphas, betas, interaction_weights = model_inter(x_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interaction_heatmap(interaction_weights, var_names=cols_weather_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the interaction heatmap show?\n",
    "The heatmap visualizes learned interaction weights between variables at each time step.\n",
    "\n",
    "Each cell (i, j) represents the strength of interaction from variable j to variable i as learned by your attention mechanism.\n",
    "\n",
    "Values are usually normalized attention scores or learned weights indicating how much one variable influences another in the model’s internal state.\n",
    "\n",
    "Why are diagonal values not 1?\n",
    "Not a simple correlation or identity matrix:\n",
    "The interaction weights are learned parameters, not raw correlation coefficients or similarity scores. They are attention scores that reflect relative importance or influence rather than a strict self-correlation.\n",
    "\n",
    "Values are normalized:\n",
    "Attention scores (like your alphas or interaction matrix) are often normalized with softmax or a similar function. This means the sum across a row or column may be 1, but individual values won’t necessarily be 1.\n",
    "\n",
    "Diagonal isn’t forced to be 1:\n",
    "Unlike correlation matrices where the diagonal is 1 by definition (variable perfectly correlated with itself), attention weights don’t have to be maximal on the diagonal. The model can learn that some variables might be more influenced by other variables than by themselves at certain time steps.\n",
    "\n",
    "Scale depends on the model’s learned patterns:\n",
    "If the diagonal is lower than 1, it might mean the model found other variables more informative or that self-interaction is not as dominant.\n",
    "\n",
    "How to interpret values like 0.02 to 0.35?\n",
    "Values closer to 0 mean weak or no interaction between those variables.\n",
    "\n",
    "Higher values (like 0.3-0.35) indicate stronger learned influence.\n",
    "\n",
    "Check if rows sum to 1 (or close), which is typical with attention weights — the heatmap shows relative importance.\n",
    "\n",
    "Additional tips:\n",
    "Compare interactions over time: If your interaction weights have a time dimension, look at how these weights evolve.\n",
    "\n",
    "Interpret in context: High interaction weight from variable A to B means the model is using variable A’s info when updating variable B’s hidden state or prediction at that step.\n",
    "\n",
    "Plot diagonal separately if needed: To see self-influence more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_weights shape: [batch, time, input_dim, input_dim]\n",
    "avg_interactions_time = interaction_weights.mean(dim=0)  # shape: [time, input_dim, input_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "time_steps_to_plot = [0, 5, 9]  # example timesteps\n",
    "\n",
    "for t in time_steps_to_plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(avg_interactions_time[t].cpu().detach().numpy(), \n",
    "                xticklabels=cols_weather_multi, yticklabels=cols_weather_multi,\n",
    "                cmap=\"viridis\", vmin=0, vmax=avg_interactions_time.max().item())\n",
    "    plt.title(f\"Variable Interactions at Time Step {t}\")\n",
    "    plt.xlabel(\"From Variable\")\n",
    "    plt.ylabel(\"To Variable\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, interaction from variable 2 to variable 5 over time\n",
    "var_from, var_to = 2, 5\n",
    "interaction_over_time = avg_interactions_time[:, var_to, var_from].cpu().detach().numpy()\n",
    "\n",
    "plt.plot(interaction_over_time)\n",
    "plt.title(f\"Interaction Weight from Var {var_from} to Var {var_to} Over Time\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Interaction Strength\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_interactions = avg_interactions_time[:, torch.arange(avg_interactions_time.size(1)), torch.arange(avg_interactions_time.size(1))]\n",
    "# or simpler:\n",
    "self_interactions = avg_interactions_time.diagonal(dim1=1, dim2=2)  # shape: [time, input_dim]\n",
    "\n",
    "for i, var_name in enumerate(cols_weather_multi):\n",
    "    plt.plot(self_interactions[:, i].cpu().detach().numpy(), label=var_name)\n",
    "\n",
    "plt.title(\"Self-Interaction Strength Over Time\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Self-Interaction Strength\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_interactions_from_variable(interactions_time, from_var_idx, var_names, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plot interaction weights over time from one variable to all others.\n",
    "\n",
    "    Args:\n",
    "        interactions_time: Tensor of shape [time, input_dim, input_dim] - averaged over batch\n",
    "        from_var_idx: int - index of the variable from which interactions originate\n",
    "        var_names: list of str - variable names matching input_dim\n",
    "        figsize: tuple - figure size\n",
    "    \n",
    "    \"\"\"\n",
    "    time_steps = interactions_time.size(0)\n",
    "    input_dim = interactions_time.size(1)\n",
    "\n",
    "    n_cols = 4\n",
    "    n_rows = int(np.ceil(input_dim / n_cols))\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for to_var_idx in range(input_dim):\n",
    "        plt.subplot(n_rows, n_cols, to_var_idx + 1)\n",
    "        y_vals = interactions_time[:, to_var_idx, from_var_idx].cpu().detach().numpy()\n",
    "        plt.plot(range(time_steps), y_vals)\n",
    "        plt.title(f\"{var_names[from_var_idx]} → {var_names[to_var_idx]}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Interaction Strength\")\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.suptitle(f\"Interactions over Time from '{var_names[from_var_idx]}' to other variables\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "# avg_interactions_time shape: [time, input_dim, input_dim], e.g. from your interaction_weights.mean(dim=0)\n",
    "from_var_idx = 1  # e.g. variable 1 (0-based index)\n",
    "plot_interactions_from_variable(avg_interactions_time, from_var_idx, cols_weather_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
