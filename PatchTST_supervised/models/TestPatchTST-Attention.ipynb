{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cbacc7-7dad-49d8-b53b-538ab80268fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from -r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from -r requirements.txt (line 2)) (3.10.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from -r requirements.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from -r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from -r requirements.txt (line 5)) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from torch==1.11.0->-r requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a450cc-e029-49b8-869a-6e2a3e1c750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\Documents\\PatchTST\\PatchTST_supervised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "#%cd C:/Users/miche/Documents/PatchTST/PatchTST_supervised\n",
    "#!bash ./scripts/PatchTST/weather_attention.sh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd48dfd-a0ab-4d0a-b7c8-2ab0e075f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\Documents\\PatchTST\\PatchTST_supervised\\scripts\\PatchTST\n",
      "Running experiment with pred_len=96, logging to ./logs/LongForecasting/PatchTST_Attention_weather_336_96.log ...\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='weather_336_96', model='PatchTST_Attention', data='custom', root_path='C:/Users/miche/Documents/PatchTST/PatchTST_supervised/dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=True, do_predict=False, num_workers=4, itr=1, train_epochs=1, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use CPU\n",
      ">>>>>>>start training : weather_336_96_PatchTST_Attention_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>testing : weather_336_96_PatchTST_Attention_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 10444\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/miche/Documents/PatchTST/PatchTST_supervised/scripts/PatchTST\n",
    "%run weather.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02bb38c-a9d4-4f66-b134-e6adcd0080b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\anaconda3\\envs\\patchtst\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\Documents\\PatchTST\\PatchTST_supervised\\scripts\\PatchTST\n",
      "Running experiment with pred_len=48, logging to ./logs/LongForecasting/PatchTST_Attention_weather_jena_84_48.log ...\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='weather_jena_84_48', model='PatchTST_Attention', data='custom', root_path='C:/Users/miche/Documents/PatchTST/PatchTST_supervised/dataset/', data_path='jena_climate_2014_2016_40min.csv', features='MS', target='T (degC)', freq='h', checkpoints='./checkpoints/', seq_len=84, label_len=48, pred_len=48, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=1, embed_type=0, enc_in=14, dec_in=14, c_out=1, d_model=140, n_heads=14, e_layers=3, d_layers=1, d_ff=560, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=True, do_predict=True, num_workers=8, itr=1, train_epochs=100, batch_size=128, patience=15, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use CPU\n",
      ">>>>>>>start training : weather_jena_84_48_PatchTST_Attention_custom_ftMS_sl84_ll48_pl48_dm140_nh14_el3_dl1_df560_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 27400\n",
      "val 3887\n",
      "test 7819\n",
      "\titers: 100, epoch: 1 | loss: 0.3278066\n",
      "\tspeed: 4.2306s/iter; left time: 90115.5395s\n",
      "\titers: 200, epoch: 1 | loss: 0.2608343\n",
      "\tspeed: 3.8218s/iter; left time: 81026.3849s\n",
      "Epoch: 1 cost time: 861.2943711280823\n",
      "Epoch: 1, Steps: 214 | Train Loss: 0.3116785 Vali Loss: 0.2170220 Test Loss: 0.2361285\n",
      "Validation loss decreased (inf --> 0.217022).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1372787\n",
      "\tspeed: 6.1601s/iter; left time: 129897.2079s\n",
      "\titers: 200, epoch: 2 | loss: 0.1427504\n",
      "\tspeed: 3.8284s/iter; left time: 80346.4610s\n",
      "Epoch: 2 cost time: 858.3216755390167\n",
      "Epoch: 2, Steps: 214 | Train Loss: 0.1774086 Vali Loss: 0.1296728 Test Loss: 0.1346051\n",
      "Validation loss decreased (0.217022 --> 0.129673).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.1543202\n",
      "\tspeed: 6.2780s/iter; left time: 131039.7615s\n",
      "\titers: 200, epoch: 3 | loss: 0.1345070\n",
      "\tspeed: 3.8594s/iter; left time: 80171.9505s\n",
      "Epoch: 3 cost time: 869.3909428119659\n",
      "Epoch: 3, Steps: 214 | Train Loss: 0.1602244 Vali Loss: 0.1258656 Test Loss: 0.1322652\n",
      "Validation loss decreased (0.129673 --> 0.125866).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.1737893\n",
      "\tspeed: 6.2619s/iter; left time: 129364.5959s\n",
      "\titers: 200, epoch: 4 | loss: 0.1567474\n",
      "\tspeed: 3.8600s/iter; left time: 79357.0694s\n",
      "Epoch: 4 cost time: 867.767945766449\n",
      "Epoch: 4, Steps: 214 | Train Loss: 0.1574374 Vali Loss: 0.1267456 Test Loss: 0.1334427\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1683277\n",
      "\tspeed: 6.1654s/iter; left time: 126051.0666s\n",
      "\titers: 200, epoch: 5 | loss: 0.1649776\n",
      "\tspeed: 3.8274s/iter; left time: 77868.0896s\n",
      "Epoch: 5 cost time: 858.0681927204132\n",
      "Epoch: 5, Steps: 214 | Train Loss: 0.1555133 Vali Loss: 0.1287613 Test Loss: 0.1351122\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1405702\n",
      "\tspeed: 6.2437s/iter; left time: 126315.4668s\n",
      "\titers: 200, epoch: 6 | loss: 0.1544818\n",
      "\tspeed: 3.8342s/iter; left time: 77186.8488s\n",
      "Epoch: 6 cost time: 862.4236810207367\n",
      "Epoch: 6, Steps: 214 | Train Loss: 0.1542651 Vali Loss: 0.1256929 Test Loss: 0.1334610\n",
      "Validation loss decreased (0.125866 --> 0.125693).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1532503\n",
      "\tspeed: 6.1546s/iter; left time: 123197.4320s\n",
      "\titers: 200, epoch: 7 | loss: 0.1835998\n",
      "\tspeed: 3.7887s/iter; left time: 75459.8260s\n",
      "Epoch: 7 cost time: 851.3518779277802\n",
      "Epoch: 7, Steps: 214 | Train Loss: 0.1534182 Vali Loss: 0.1265860 Test Loss: 0.1336847\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1521390\n",
      "\tspeed: 6.1810s/iter; left time: 122401.6008s\n",
      "\titers: 200, epoch: 8 | loss: 0.1251147\n",
      "\tspeed: 3.8293s/iter; left time: 75449.5775s\n",
      "Epoch: 8 cost time: 859.1488952636719\n",
      "Epoch: 8, Steps: 214 | Train Loss: 0.1517924 Vali Loss: 0.1256215 Test Loss: 0.1333528\n",
      "Validation loss decreased (0.125693 --> 0.125621).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.1469586\n",
      "\tspeed: 6.1531s/iter; left time: 120532.1490s\n",
      "\titers: 200, epoch: 9 | loss: 0.1374888\n",
      "\tspeed: 3.8346s/iter; left time: 74732.2400s\n",
      "Epoch: 9 cost time: 859.3353841304779\n",
      "Epoch: 9, Steps: 214 | Train Loss: 0.1516349 Vali Loss: 0.1252190 Test Loss: 0.1327112\n",
      "Validation loss decreased (0.125621 --> 0.125219).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.1507970\n",
      "\tspeed: 6.2112s/iter; left time: 120341.8933s\n",
      "\titers: 200, epoch: 10 | loss: 0.1516492\n",
      "\tspeed: 3.9142s/iter; left time: 75446.7051s\n",
      "Epoch: 10 cost time: 871.0803356170654\n",
      "Epoch: 10, Steps: 214 | Train Loss: 0.1508306 Vali Loss: 0.1244457 Test Loss: 0.1321839\n",
      "Validation loss decreased (0.125219 --> 0.124446).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.1377597\n",
      "\tspeed: 6.1071s/iter; left time: 117018.6596s\n",
      "\titers: 200, epoch: 11 | loss: 0.1640022\n",
      "\tspeed: 3.8730s/iter; left time: 73824.1117s\n",
      "Epoch: 11 cost time: 867.5600845813751\n",
      "Epoch: 11, Steps: 214 | Train Loss: 0.1502684 Vali Loss: 0.1236309 Test Loss: 0.1318523\n",
      "Validation loss decreased (0.124446 --> 0.123631).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.1579429\n",
      "\tspeed: 6.0151s/iter; left time: 113967.9300s\n",
      "\titers: 200, epoch: 12 | loss: 0.1306036\n",
      "\tspeed: 3.8376s/iter; left time: 72327.9669s\n",
      "Epoch: 12 cost time: 856.4474821090698\n",
      "Epoch: 12, Steps: 214 | Train Loss: 0.1493851 Vali Loss: 0.1257290 Test Loss: 0.1330395\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.1360450\n",
      "\tspeed: 6.1016s/iter; left time: 114301.5588s\n",
      "\titers: 200, epoch: 13 | loss: 0.1174149\n",
      "\tspeed: 3.9658s/iter; left time: 73894.4616s\n",
      "Epoch: 13 cost time: 877.6161820888519\n",
      "Epoch: 13, Steps: 214 | Train Loss: 0.1490790 Vali Loss: 0.1243975 Test Loss: 0.1322290\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.1592845\n",
      "\tspeed: 6.4085s/iter; left time: 118679.6898s\n",
      "\titers: 200, epoch: 14 | loss: 0.1298224\n",
      "\tspeed: 3.9564s/iter; left time: 72872.0722s\n",
      "Epoch: 14 cost time: 890.0752527713776\n",
      "Epoch: 14, Steps: 214 | Train Loss: 0.1484623 Vali Loss: 0.1248767 Test Loss: 0.1331041\n",
      "EarlyStopping counter: 3 out of 15\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.1456644\n",
      "\tspeed: 6.4039s/iter; left time: 117223.8368s\n",
      "\titers: 200, epoch: 15 | loss: 0.1434957\n",
      "\tspeed: 3.9582s/iter; left time: 72059.5666s\n",
      "Epoch: 15 cost time: 889.1413078308105\n",
      "Epoch: 15, Steps: 214 | Train Loss: 0.1481990 Vali Loss: 0.1257233 Test Loss: 0.1339251\n",
      "EarlyStopping counter: 4 out of 15\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.1355718\n",
      "\tspeed: 6.3757s/iter; left time: 115342.2655s\n",
      "\titers: 200, epoch: 16 | loss: 0.1664172\n",
      "\tspeed: 3.9367s/iter; left time: 70825.5621s\n",
      "Epoch: 16 cost time: 882.1217756271362\n",
      "Epoch: 16, Steps: 214 | Train Loss: 0.1475913 Vali Loss: 0.1244007 Test Loss: 0.1321894\n",
      "EarlyStopping counter: 5 out of 15\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.1595998\n",
      "\tspeed: 6.0384s/iter; left time: 107948.6980s\n",
      "\titers: 200, epoch: 17 | loss: 0.1510556\n",
      "\tspeed: 3.9262s/iter; left time: 69796.2434s\n",
      "Epoch: 17 cost time: 865.9879026412964\n",
      "Epoch: 17, Steps: 214 | Train Loss: 0.1474230 Vali Loss: 0.1256669 Test Loss: 0.1337055\n",
      "EarlyStopping counter: 6 out of 15\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.1698953\n",
      "\tspeed: 6.1024s/iter; left time: 107787.4357s\n",
      "\titers: 200, epoch: 18 | loss: 0.1611830\n",
      "\tspeed: 3.8707s/iter; left time: 67980.3750s\n",
      "Epoch: 18 cost time: 860.368424654007\n",
      "Epoch: 18, Steps: 214 | Train Loss: 0.1470909 Vali Loss: 0.1246835 Test Loss: 0.1324802\n",
      "EarlyStopping counter: 7 out of 15\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.1356178\n",
      "\tspeed: 6.0796s/iter; left time: 106083.1417s\n",
      "\titers: 200, epoch: 19 | loss: 0.1434191\n",
      "\tspeed: 3.9188s/iter; left time: 67987.9773s\n",
      "Epoch: 19 cost time: 871.5874242782593\n",
      "Epoch: 19, Steps: 214 | Train Loss: 0.1467541 Vali Loss: 0.1250998 Test Loss: 0.1330418\n",
      "EarlyStopping counter: 8 out of 15\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.1669043\n",
      "\tspeed: 6.2661s/iter; left time: 107996.6034s\n",
      "\titers: 200, epoch: 20 | loss: 0.1354423\n",
      "\tspeed: 3.8531s/iter; left time: 66022.5328s\n",
      "Epoch: 20 cost time: 869.8115265369415\n",
      "Epoch: 20, Steps: 214 | Train Loss: 0.1464013 Vali Loss: 0.1256684 Test Loss: 0.1332304\n",
      "EarlyStopping counter: 9 out of 15\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.1279707\n",
      "\tspeed: 6.0088s/iter; left time: 102275.0922s\n",
      "\titers: 200, epoch: 21 | loss: 0.1581376\n",
      "\tspeed: 3.8331s/iter; left time: 64860.0383s\n",
      "Epoch: 21 cost time: 854.8973803520203\n",
      "Epoch: 21, Steps: 214 | Train Loss: 0.1461676 Vali Loss: 0.1255994 Test Loss: 0.1331262\n",
      "EarlyStopping counter: 10 out of 15\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.1488991\n",
      "\tspeed: 6.0592s/iter; left time: 101837.7721s\n",
      "\titers: 200, epoch: 22 | loss: 0.1324001\n",
      "\tspeed: 3.8858s/iter; left time: 64919.4815s\n",
      "Epoch: 22 cost time: 864.125506401062\n",
      "Epoch: 22, Steps: 214 | Train Loss: 0.1463660 Vali Loss: 0.1244908 Test Loss: 0.1322461\n",
      "EarlyStopping counter: 11 out of 15\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.1567296\n",
      "\tspeed: 6.0141s/iter; left time: 99791.6982s\n",
      "\titers: 200, epoch: 23 | loss: 0.1422524\n",
      "\tspeed: 3.8159s/iter; left time: 62935.6921s\n",
      "Epoch: 23 cost time: 853.8103561401367\n",
      "Epoch: 23, Steps: 214 | Train Loss: 0.1458158 Vali Loss: 0.1239725 Test Loss: 0.1323074\n",
      "EarlyStopping counter: 12 out of 15\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.1671256\n",
      "\tspeed: 6.1601s/iter; left time: 100896.5551s\n",
      "\titers: 200, epoch: 24 | loss: 0.1440105\n",
      "\tspeed: 3.9071s/iter; left time: 63604.1786s\n",
      "Epoch: 24 cost time: 868.336843252182\n",
      "Epoch: 24, Steps: 214 | Train Loss: 0.1459929 Vali Loss: 0.1247732 Test Loss: 0.1327818\n",
      "EarlyStopping counter: 13 out of 15\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.1714056\n",
      "\tspeed: 6.1244s/iter; left time: 99000.1490s\n",
      "\titers: 200, epoch: 25 | loss: 0.1412122\n",
      "\tspeed: 3.9198s/iter; left time: 62970.8814s\n",
      "Epoch: 25 cost time: 872.89705991745\n",
      "Epoch: 25, Steps: 214 | Train Loss: 0.1458087 Vali Loss: 0.1252256 Test Loss: 0.1330932\n",
      "EarlyStopping counter: 14 out of 15\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.1653918\n",
      "\tspeed: 6.1110s/iter; left time: 97477.0459s\n",
      "\titers: 200, epoch: 26 | loss: 0.1376508\n",
      "\tspeed: 3.9331s/iter; left time: 62342.8146s\n",
      "Epoch: 26 cost time: 874.329770565033\n",
      "Epoch: 26, Steps: 214 | Train Loss: 0.1455788 Vali Loss: 0.1246496 Test Loss: 0.1324197\n",
      "EarlyStopping counter: 15 out of 15\n",
      "Early stopping\n",
      ">>>>>>>testing : weather_jena_84_48_PatchTST_Attention_custom_ftMS_sl84_ll48_pl48_dm140_nh14_el3_dl1_df560_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 7819\n",
      "mse:0.13185226917266846, mae:0.27274537086486816, rse:0.3410506546497345\n",
      ">>>>>>>predicting : weather_jena_84_48_PatchTST_Attention_custom_ftMS_sl84_ll48_pl48_dm140_nh14_el3_dl1_df560_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "pred 1\n",
      "[Predict] Started prediction\n",
      "Loading model from: ./checkpoints/weather_jena_84_48_PatchTST_Attention_custom_ftMS_sl84_ll48_pl48_dm140_nh14_el3_dl1_df560_fc1_ebtimeF_dtTrue_Exp_0\\checkpoint.pth\n",
      "[Predict] Model checkpoint loaded successfully\n",
      "Predictions saved to: ./results/weather_jena_84_48_PatchTST_Attention_custom_ftMS_sl84_ll48_pl48_dm140_nh14_el3_dl1_df560_fc1_ebtimeF_dtTrue_Exp_0/pred.npy\n",
      "Ground truth saved to: ./results/weather_jena_84_48_PatchTST_Attention_custom_ftMS_sl84_ll48_pl48_dm140_nh14_el3_dl1_df560_fc1_ebtimeF_dtTrue_Exp_0/true.npy\n",
      "Experiment for pred_len=48 finished.\n",
      "\n",
      "All experiments completed.\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/miche/Documents/PatchTST/PatchTST_supervised/scripts/PatchTST\n",
    "%run weather_jena.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf72e95-ed34-4f96-8434-c68eae76cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Your model definition (make sure this matches how you trained it)\n",
    "from your_model_file import PatchTST  # modify as needed\n",
    "\n",
    "model = PatchTST(...)\n",
    "model.load_state_dict(torch.load(\"checkpoint.pth\", map_location='cpu'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f4607a-1db6-4fb1-b3af-743cfd0591fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(batch_input_np):\n",
    "    \"\"\"\n",
    "    TimeSHAP expects input: np.ndarray of shape (B, T, C)\n",
    "    PatchTST expects input: torch.Tensor (B, T, C)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(batch_input_np, dtype=torch.float32)\n",
    "        pred = model(input_tensor)  # shape: (B, pred_len, C)\n",
    "        # Extract the first prediction step and first variable\n",
    "        return pred[:, 0, 0].cpu().numpy()  # Modify as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c619d7ee-efe4-4181-bc60-365e0b1a6f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\miche\\Documents\\PatchTST\\PatchTST_supervised\n",
      "Now in: C:\\Users\\miche\\Documents\\PatchTST\\PatchTST_supervised\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "os.chdir('C:/Users/miche/Documents/PatchTST/PatchTST_supervised/')\n",
    "print(\"Now in:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a73c30-fb6f-48cc-b15d-040ba715c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "seq_len=84\n",
    "label_len=48\n",
    "pred_len=48\n",
    "dataset = Dataset_Custom(\n",
    "    root_path='C:/Users/miche/Documents/PatchTST/PatchTST_supervised/dataset/',\n",
    "    flag='test',\n",
    "    size=[seq_len, label_len, pred_len],\n",
    "    features='MS',\n",
    "    data_path='jena_climate_2014_2016_40min.csv',\n",
    "    target='T (degC)',\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "sample = next(iter(loader))\n",
    "\n",
    "seq_x, seq_y, seq_x_mark, seq_y_mark = sample  # each is shape [1, T, C] or similar\n",
    "input_sample = seq_x.squeeze(0).numpy()        # shape (T, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82aad825-ed8d-4d64-9cdc-21fb7b5f6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # random seed\n",
    "    random_seed = 2021\n",
    "\n",
    "    # basic config\n",
    "    is_training = 1\n",
    "    model_id = 'test'\n",
    "    model = 'PatchTST_Attention'\n",
    "\n",
    "    # data loader\n",
    "    data = 'ETTm1'\n",
    "    root_path = './data/ETT/'\n",
    "    data_path = 'ETTh1.csv'\n",
    "    features = 'MS'\n",
    "    target = 'T (degC)'\n",
    "    freq = 'h'\n",
    "    checkpoints = './checkpoints/'\n",
    "\n",
    "    # forecasting task\n",
    "    seq_len = 84\n",
    "    label_len = 48\n",
    "    pred_len = 48\n",
    "\n",
    "    # PatchTST\n",
    "    fc_dropout = 0.2\n",
    "    head_dropout = 0.0\n",
    "    patch_len = 16\n",
    "    stride = 8\n",
    "    padding_patch = 'end'\n",
    "    revin = 1\n",
    "    affine = 0\n",
    "    subtract_last = 0\n",
    "    decomposition = 0\n",
    "    kernel_size = 25\n",
    "    individual = 1  # True\n",
    "\n",
    "    # Formers\n",
    "    embed_type = 0\n",
    "    enc_in = 14\n",
    "    dec_in = 14\n",
    "    c_out = 1\n",
    "    d_model = 140\n",
    "    n_heads = 14\n",
    "    e_layers = 3\n",
    "    d_layers = 1\n",
    "    d_ff = 560\n",
    "    moving_avg = 25\n",
    "    factor = 1\n",
    "    distil = True\n",
    "    dropout = 0.2\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    output_attention = True\n",
    "    do_predict = False\n",
    "\n",
    "    # optimization\n",
    "    num_workers = 8\n",
    "    itr = 1\n",
    "    train_epochs = 50\n",
    "    batch_size = 128\n",
    "    patience = 5\n",
    "    learning_rate = 0.0001\n",
    "    des = 'Exp'\n",
    "    loss = 'mse'\n",
    "    lradj = 'type3'\n",
    "    pct_start = 0.3\n",
    "    use_amp = False\n",
    "\n",
    "    # GPU\n",
    "    use_gpu = True\n",
    "    gpu = 0\n",
    "    use_multi_gpu = False\n",
    "    devices = '0,1,2,3'\n",
    "    test_flop = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b18b7d50-49fb-40b5-8d96-ed6a10bbcdfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PatchTST_Attention\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize (use the correct args you used for training)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPatchTST_Attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load weights if needed\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_model_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# Example: assuming your model was initialized and loaded like this\n",
    "from models import PatchTST_Attention\n",
    "\n",
    "# Initialize (use the correct args you used for training)\n",
    "model = PatchTST_Attention(args).to(device)\n",
    "\n",
    "# Load weights if needed\n",
    "model.load_state_dict(torch.load('path_to_model_checkpoint.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "508d64f4-4bd6-41af-9f10-d892ab7c4e61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ---- Load your model (already loaded as `model`) ----\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ---- Prediction wrapper ----\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_fn\u001b[39m(batch_input_np):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Load your model (already loaded as `model`) ----\n",
    "model.eval()\n",
    "\n",
    "# ---- Prediction wrapper ----\n",
    "def predict_fn(batch_input_np):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(batch_input_np, dtype=torch.float32)\n",
    "        output = model(input_tensor)  # shape: (B, pred_len, C)\n",
    "        return output[:, 0, 0].cpu().numpy()  # predicting just one step and one feature\n",
    "\n",
    "# ---- Load dataset ----\n",
    "seq_len = 84\n",
    "label_len = 48\n",
    "pred_len = 48\n",
    "\n",
    "dataset = Dataset_Custom(\n",
    "    root_path='C:/Users/miche/Documents/PatchTST/PatchTST_supervised/dataset/',\n",
    "    flag='test',\n",
    "    size=[seq_len, label_len, pred_len],\n",
    "    features='MS',\n",
    "    data_path='jena_climate_2014_2016_40min.csv',\n",
    "    target='T (degC)',\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# ---- Prepare data to explain ----\n",
    "X = []\n",
    "for i, sample in enumerate(loader):\n",
    "    if i >= 20:\n",
    "        break\n",
    "    seq_x, _, _, _ = sample\n",
    "    X.append(seq_x.squeeze(0).numpy())\n",
    "\n",
    "X = np.stack(X)  # shape: (20, T, C)\n",
    "X_background = X[:10]\n",
    "X_to_explain = X[10:11]\n",
    "\n",
    "# ---- Build SHAP KernelExplainer ----\n",
    "explainer = shap.KernelExplainer(predict_fn, X_background)\n",
    "\n",
    "# ---- Get SHAP values ----\n",
    "shap_values = explainer.shap_values(X_to_explain, nsamples=100)\n",
    "\n",
    "# ---- Visualize ----\n",
    "# shap_values has shape: (T * C,)\n",
    "T, C = X_to_explain.shape[1:]  # (T, C)\n",
    "shap_array = np.array(shap_values).reshape(T, C).T  # shape: (C, T)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(shap_array, aspect='auto', cmap='bwr')\n",
    "plt.colorbar(label='SHAP value')\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"SHAP Explanation for PatchTST Prediction\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a9a189f-6f57-4e86-9c05-631e540cbac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Callable', 'List', 'Path', 'TimeShapKernel', 'Tuple', 'Union', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'cell_level', 'cell_top_events', 'cell_top_feats', 'considered_cells', 'copy', 'csv', 'event_explain_all', 'event_level', 'feat_explain_all', 'feature_level', 'global_methods', 'global_report', 'kernel', 'local_cell_level', 'local_event', 'local_feat', 'local_methods', 'local_pruning', 'local_report', 'np', 'os', 'pd', 'plot_cell_level', 'plot_event_heatmap', 'plot_feat_barplot', 'plot_global_event', 'plot_global_feat', 'plot_temp_coalition_pruning', 'prune_all', 'prune_given_data', 'pruning', 'pruning_statistics', 're', 'temp_coalition_pruning', 'timeshap_kernel', 'validate_global_input', 'validate_local_input']\n"
     ]
    }
   ],
   "source": [
    "import timeshap.explainer\n",
    "print(dir(timeshap.explainer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b4a10-9baa-440b-a66e-ec5c5857f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot SHAP values (aggregated or per-feature)\n",
    "plt.plot(shap_values)\n",
    "plt.title(\"TimeSHAP values\")\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"SHAP value\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (patchtst)",
   "language": "python",
   "name": "patchtst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
